{
	"version": "https://jsonfeed.org/version/1",
	"title": "Brash \u0026 Plucky",
	"icon": "https://micro.blog/brashandplucky/avatar.jpg",
	"home_page_url": "https://www.brashandplucky.com/",
	"feed_url": "https://www.brashandplucky.com/feed.json",
	"items": [
		
			{
				"id": "http://brashandplucky.micro.blog/2022/05/05/float-int-quiz.html",
				"title": "Recklessly encoding int as float",
				"content_html": "<p><em>Quiz:</em> Does this loop end, and if it does, what is the value of <code>k</code> after the loop? (assuming IEEE-754 and usual 32-bit semantics for <code>int/float</code>)</p>\n\n<p><img src=\"uploads/2022/cc2287c141.png\" width=\"600\" height=\"155\" alt=\"\" /></p>\n\n<p>Running this code in VS you get <code>k=16777217</code> which equals <code>2^24+1</code>.</p>\n\n<p>In other words, in line with the true intent of the quiz, <code>float</code> can encode -exactly- (without any precission loss) all natural numbers up to <code>2^24</code> (included). Because <code>float</code> encodes sign in a dedicated bit, this property holds true for negative values as well. So you could <em>hypothetically</em> encode any <code>[-2^24..+2^24]</code> <code>int</code> in a <code>float</code> with a static/implicit cast.</p>\n\n<p>I said <em>hypothetically</em> because generally speaking I would never do such thing. Carelessly casting between <code>int</code> and <code>float</code> (without an explicit <code>floor</code>, <code>ceil</code>, <code>trunc</code>, <code>round</code>&hellip;) is often a sign of lousy coding, unless performance is at stake and you know well what you are doing.</p>\n\n<p>However, I came across the situation in Unity recently, where the <code>Vertex ID</code> node in their Shader Graph hands over <code>SV_VertexID</code> as a <code>float</code> (disappointed face :-P). I would&rsquo;ve expected an <code>uint/int</code> output or a <code>float</code> that you could reinterpret-cast to retrieve the raw <code>SV_VertexID</code> bits with <a href=\"https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asuint\">asuint()</a>. But nope. You get a <code>float</code> which seemingly carries <code>(float)SV_VertexID</code>.</p>\n\n<p>One may recklessly use this <code>float</code> to index a <code>StructuredBuffer</code> ignoring the way static-casting works. This is one case where ignorance is bliss, because the <code>float</code> received exactly matches the original <code>int</code> as long as the original value is <code>&lt;=2^24</code>. That is, as long as you are dealing with fewer than (roughly) 16.7 million vertices, which is usually the case.</p>\n\n<p>I believe that Unity&rsquo;s ShaderGraph doesn&rsquo;t support <code>int/uint</code> as In/Out values between nodes, so I guess that the <code>Vertex ID</code> and <code>Instance ID</code> nodes just static-cast the corresponding <code>SV_...</code> value to a float. But it would be better (in the <em>pedantic</em> sense) to reinterpret the bit pattern of the raw values with <code>asfloat</code> and then let the user retrieve them with <code>asint()/asuint()</code>.</p>\n\n<p>&ndash;</p>\n\n<p><strong>reinterpret-cast between int/float in HSLS:</strong></p>\n\n<p><a href=\"https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asint\">asint()</a>\n<a href=\"https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asuint\">asuint()</a>\n<a href=\"https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asfloat\">asfloat()</a></p>\n\n<p>&ndash;</p>\n\n<p><strong>A (loose) explanation of the 2^24 limit:</strong></p>\n\n<p>This is the <a href=\"https://en.wikipedia.org/wiki/IEEE_754\">IEEE-754 standard</a> for floating-point numbers, as described in Wikipedia:</p>\n\n<p><img src=\"uploads/2022/c76210ac43.png\" width=\"600\" height=\"90\" alt=\"\" /></p>\n\n<p>The value of a float-encoded number is reconstructed as <code>(-1)^S * M * 2^E</code>.</p>\n\n<ul>\n<li><code>S</code> is the 1-bit sign.</li>\n<li><code>M</code> is the 23-bit mantissa, interpreted as <code>1.xxxxx</code> (in binary).</li>\n<li><code>E</code> is the 8-bit exponent, used as <code>E-127</code> where 127 is often called <em>bias</em>.</li>\n</ul>\n\n<p><em>e.g.,</em> a power-of-2 integer number like 4 would be encoded as:</p>\n\n<ul>\n<li><code>M=1.00</code> which is the binary representation of 4, with the point at the left-most 1.</li>\n<li><code>E=2</code> (+bias).</li>\n</ul>\n\n<p>The restored number is <code>1.00 shl 2=4</code>.</p>\n\n<p>We should be able to do the same for all power-of-2 integers until we max out <code>E</code>.</p>\n\n<p>For non-power-of-2 integers the process is similar. <em>e.g.,</em> number 5:</p>\n\n<ul>\n<li><code>M=1.01...00</code>.</li>\n<li><code>E=2</code> (+bias).</li>\n</ul>\n\n<p>The restored number is now <code>1.01 shl 2=5</code>.</p>\n\n<p>This works the same for all integer numbers as long as <code>M</code> can hold the raw binary representation of the number. The tallest binary number that <code>M</code> can hold is 23 consecutive 1s. That is: 1.11&hellip;11 (24x1s in total). With <code>E=23</code> (+bias) this equals <code>2^24-1</code>.</p>\n\n<p>The next integer <code>2^24</code> would be encoded as <code>1.00...00</code> (24x0s, clamped to 23, but the trailing 0s are meaningless here). With <code>E=24</code> (+bias) this equals <code>2^24</code> (<strong>the answer provided above!!</strong>).</p>\n\n<p>But the next integer <code>1.{23x0s.1}</code> can&rsquo;t be encoded in a 23-bit <code>M</code>. So from 2^24 onwards, there is necessarily a loss. Some integers beyond 2^24 (such as powers-of-2) will be luckily encoded exactly by <code>float</code>. But not all consecutive numbers will. Actually, <code>2^24+1</code> is the first integer that won&rsquo;t.</p>\n\n<p>As always, <em>apologies for any mistakes in any of my posts</em>.</p>\n\n<p>&ndash;</p>\n\n<p><strong>[EDIT]</strong> The same reasoning can be applied to double, where the mantisa <code>M</code> is 52-bit long.</p>\n\n<p><img src=\"uploads/2022/f0b41d494d.png\" width=\"600\" height=\"130\" alt=\"\" /></p>\n\n<p>Hence double can encode exactly all integers in the range <code>[-2^53..+2^53]</code>.</p>\n",
				"content_text": "_Quiz:_ Does this loop end, and if it does, what is the value of `k` after the loop? (assuming IEEE-754 and usual 32-bit semantics for `int/float`)\n\n<img src=\"uploads/2022/cc2287c141.png\" width=\"600\" height=\"155\" alt=\"\" />\n\nRunning this code in VS you get `k=16777217` which equals `2^24+1`.\n\nIn other words, in line with the true intent of the quiz, `float` can encode -exactly- (without any precission loss) all natural numbers up to `2^24` (included). Because `float` encodes sign in a dedicated bit, this property holds true for negative values as well. So you could _hypothetically_ encode any `[-2^24..+2^24]` `int` in a `float` with a static/implicit cast.\n\nI said _hypothetically_ because generally speaking I would never do such thing. Carelessly casting between `int` and `float` (without an explicit `floor`, `ceil`, `trunc`, `round`...) is often a sign of lousy coding, unless performance is at stake and you know well what you are doing.\n\nHowever, I came across the situation in Unity recently, where the `Vertex ID` node in their Shader Graph hands over `SV_VertexID` as a `float` (disappointed face :-P). I would've expected an `uint/int` output or a `float` that you could reinterpret-cast to retrieve the raw `SV_VertexID` bits with [asuint()](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asuint). But nope. You get a `float` which seemingly carries `(float)SV_VertexID`.\n\nOne may recklessly use this `float` to index a `StructuredBuffer` ignoring the way static-casting works. This is one case where ignorance is bliss, because the `float` received exactly matches the original `int` as long as the original value is `<=2^24`. That is, as long as you are dealing with fewer than (roughly) 16.7 million vertices, which is usually the case.\n\nI believe that Unity's ShaderGraph doesn't support `int/uint` as In/Out values between nodes, so I guess that the `Vertex ID` and `Instance ID` nodes just static-cast the corresponding `SV_...` value to a float. But it would be better (in the _pedantic_ sense) to reinterpret the bit pattern of the raw values with `asfloat` and then let the user retrieve them with `asint()/asuint()`.\n\n--\n\n**reinterpret-cast between int/float in HSLS:**\n\n[asint()](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asint)\n[asuint()](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asuint)\n[asfloat()](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-asfloat)\n\n--\n\n**A (loose) explanation of the 2^24 limit:**\n\nThis is the [IEEE-754 standard](https://en.wikipedia.org/wiki/IEEE_754) for floating-point numbers, as described in Wikipedia:\n\n<img src=\"uploads/2022/c76210ac43.png\" width=\"600\" height=\"90\" alt=\"\" />\n\nThe value of a float-encoded number is reconstructed as `(-1)^S * M * 2^E`.\n\n- `S` is the 1-bit sign.\n- `M` is the 23-bit mantissa, interpreted as `1.xxxxx` (in binary).\n- `E` is the 8-bit exponent, used as `E-127` where 127 is often called _bias_.\n\n_e.g.,_ a power-of-2 integer number like 4 would be encoded as:\n\n- `M=1.00` which is the binary representation of 4, with the point at the left-most 1.\n- `E=2` (+bias).\n\nThe restored number is `1.00 shl 2=4`.\n\nWe should be able to do the same for all power-of-2 integers until we max out `E`.\n\nFor non-power-of-2 integers the process is similar. _e.g.,_ number 5:\n\n- `M=1.01...00`.\n- `E=2` (+bias).\n\nThe restored number is now `1.01 shl 2=5`.\n\nThis works the same for all integer numbers as long as `M` can hold the raw binary representation of the number. The tallest binary number that `M` can hold is 23 consecutive 1s. That is: 1.11...11 (24x1s in total). With `E=23` (+bias) this equals `2^24-1`.\n\nThe next integer `2^24` would be encoded as `1.00...00` (24x0s, clamped to 23, but the trailing 0s are meaningless here). With `E=24` (+bias) this equals `2^24` (**the answer provided above!!**).\n\nBut the next integer `1.{23x0s.1}` can't be encoded in a 23-bit `M`. So from 2^24 onwards, there is necessarily a loss. Some integers beyond 2^24 (such as powers-of-2) will be luckily encoded exactly by `float`. But not all consecutive numbers will. Actually, `2^24+1` is the first integer that won't.\n\nAs always, _apologies for any mistakes in any of my posts_.\n\n--\n\n**[EDIT]** The same reasoning can be applied to double, where the mantisa `M` is 52-bit long.\n\n<img src=\"uploads/2022/f0b41d494d.png\" width=\"600\" height=\"130\" alt=\"\" />\n\nHence double can encode exactly all integers in the range `[-2^53..+2^53]`.\n",
				"date_published": "2022-05-05T19:57:00+02:00",
				"url": "https://www.brashandplucky.com/2022/05/05/float-int-quiz.html",
				"tags": ["coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/04/20/fake-viewport-pbr.html",
				"title": "Viewport baked PBR materials",
				"content_html": "<p>The technique described here is very basic, but useful nonetheless.</p>\n\n<p>Let&rsquo;s make the following (cringy :-D) assumptions:</p>\n\n<ol>\n<li>No interactions between surfaces (<em>i.e.,</em> no GI, no occlusion, etc&hellip;).</li>\n<li>The only light source is an IBL (environment) map placed at infinity which orientation is anchored to the camera.</li>\n<li>Each surface point <em>P</em> is made of a perfectly rough white material which simply &ldquo;collects&rdquo; the light that arrives to <em>P</em> from above the hemiplane defined by <em>(P,N)</em>.</li>\n</ol>\n\n<p><img src=\"uploads/2022/46104cc277.png\" width=\"600\" height=\"147\" alt=\"\" /></p>\n\n<p>Under such assumptions a ball floating in the void looks like this image rendered in <a href=\"https://maverickrender.com/\">Maverick Studio</a> with a procedural sphere primitive and a standard material:</p>\n\n<p><img src=\"uploads/2022/fa47d8b7f7.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/6e5bed50ac.png\" width=\"600\" height=\"300\" alt=\"\" /></p>\n\n<p>The illumination collected by each point <em>P</em> and radiated back to the camera (+ tonemapping) happens to be, exactly, one particular pixel in the rendered ball as depicted in the hand-drawn diagram above. Note that due to the constraints set, <strong>this color depends exclusively on the normal <em>N</em> at <em>P</em> but not on <em>P</em> itself</strong>. In other words, the rendered ball effectively precomputes what any surface point <em>P</em> sees through its normal <em>N</em>.</p>\n\n<p>The goal in this post is to benefit from this knowledge in a rasterization viewport. So let&rsquo;s jump now to the world of pixel shaders. In our vertex shader, let&rsquo;s transform the object&rsquo;s normals <em>N</em> to camera space <em>Nc</em> and let&rsquo;s drop the resulting Z coordinate. <em>(Nc.x,Nc.y)</em> are the geometry normals projected to screen space. Let&rsquo;s normalize these: <em>Nc&rsquo;=normalize(Nc.x,Nc.y)</em></p>\n\n<p>Let&rsquo;s upload the ball render as a regular 2D texture and let&rsquo;s define the following UV space:</p>\n\n<p><img src=\"uploads/2022/4761754b1a.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p><em>Nc&rsquo;</em> is normalized, so the UVs generated will cover the interior of the ball only. Normals at grazing angles will be near the perimeter of the texture&rsquo;s ball.</p>\n\n<p>In our fragment shader, let&rsquo;s simply output the color found in the ball image at the UVs computed as described. Here&rsquo;s the result:</p>\n\n<p><img src=\"uploads/2022/c2abfa5373.gif\" width=\"600\" height=\"574\" alt=\"\" /></p>\n\n<p>Of course, physically-based materials are more complicated than this. In general, the amount of light received by the surface from the <em>(P,N)</em> hemiplane through each possible incoming direction and then bounced back through each possible outgoing direction depends on both directions, and also on the properties of the material. This is basically the definition of a <a href=\"https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function\">BRDF</a>, BTW.</p>\n\n<p>But because we set the constraint that the lights are not moving (<em>i.e.,</em> the IBL stays fixed with respect to the camera) the incoming and outgoing directions and hence the behavior of the BRDF ends up being a function of <em>N</em> and <em>nothing</em> else. So this method still works <strong>no matter what material we render the ball with</strong> (well&hellip; excluding refractions :-D).</p>\n\n<p>Here goes another example with the same IBL but adding some specularity to the material:</p>\n\n<p><img src=\"uploads/2022/9be65014fb.gif\" width=\"600\" height=\"574\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/c4514647d1.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p>Of course this could be pushed further with some SSAO or any other technique that approximates self-shadowing. But this is a very compact and cheap (memory/code/effort) way to display models in an easy-to-author appealing way.</p>\n\n<p>Some final notes:</p>\n\n<ul>\n<li>If you want to be accurate, the ball should be rendered with an <em>orthographic</em> camera (<em>i.e.,</em> not with a <em>perspective</em> camera).</li>\n<li>For blurry/glossy or diffuse reflections, you can get away with a low-res ball render. However, make sure that the UVs that you generate don&rsquo;t step on the antialias pixels at the boundary of the rendered ball. Otherwise the model will look b0rked at grazing angles.</li>\n<li>This technique can be extended by using 2 ball images: one with the diffuse component of the material, and another with the specular component. This way the fragment shader could modulate the diffuse part and the specular part by separate colors that could be passed as inputs to the shader. Keep in mind that this would require that the ball images and the colors are passed in linear (instead of sRGB) space and that the resulting color is sent back to sRGB.</li>\n</ul>\n",
				"content_text": "The technique described here is very basic, but useful nonetheless.\n\nLet's make the following (cringy :-D) assumptions:\n\n1. No interactions between surfaces (_i.e.,_ no GI, no occlusion, etc...).\n2. The only light source is an IBL (environment) map placed at infinity which orientation is anchored to the camera.\n3. Each surface point _P_ is made of a perfectly rough white material which simply \"collects\" the light that arrives to _P_ from above the hemiplane defined by _(P,N)_.\n\n<img src=\"uploads/2022/46104cc277.png\" width=\"600\" height=\"147\" alt=\"\" />\n\nUnder such assumptions a ball floating in the void looks like this image rendered in [Maverick Studio](https://maverickrender.com/) with a procedural sphere primitive and a standard material:\n\n<img src=\"uploads/2022/fa47d8b7f7.png\" width=\"600\" height=\"600\" alt=\"\" />\n\n<img src=\"uploads/2022/6e5bed50ac.png\" width=\"600\" height=\"300\" alt=\"\" />\n\nThe illumination collected by each point _P_ and radiated back to the camera (+ tonemapping) happens to be, exactly, one particular pixel in the rendered ball as depicted in the hand-drawn diagram above. Note that due to the constraints set, **this color depends exclusively on the normal _N_ at _P_ but not on _P_ itself**. In other words, the rendered ball effectively precomputes what any surface point _P_ sees through its normal _N_.\n\nThe goal in this post is to benefit from this knowledge in a rasterization viewport. So let's jump now to the world of pixel shaders. In our vertex shader, let's transform the object's normals _N_ to camera space _Nc_ and let's drop the resulting Z coordinate. _(Nc.x,Nc.y)_ are the geometry normals projected to screen space. Let's normalize these: _Nc'=normalize(Nc.x,Nc.y)_\n\nLet's upload the ball render as a regular 2D texture and let's define the following UV space:\n\n<img src=\"uploads/2022/4761754b1a.png\" width=\"600\" height=\"600\" alt=\"\" />\n\n_Nc'_ is normalized, so the UVs generated will cover the interior of the ball only. Normals at grazing angles will be near the perimeter of the texture's ball.\n\nIn our fragment shader, let's simply output the color found in the ball image at the UVs computed as described. Here's the result:\n\n<img src=\"uploads/2022/c2abfa5373.gif\" width=\"600\" height=\"574\" alt=\"\" />\n\nOf course, physically-based materials are more complicated than this. In general, the amount of light received by the surface from the _(P,N)_ hemiplane through each possible incoming direction and then bounced back through each possible outgoing direction depends on both directions, and also on the properties of the material. This is basically the definition of a [BRDF](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function), BTW.\n\nBut because we set the constraint that the lights are not moving (_i.e.,_ the IBL stays fixed with respect to the camera) the incoming and outgoing directions and hence the behavior of the BRDF ends up being a function of _N_ and _nothing_ else. So this method still works **no matter what material we render the ball with** (well... excluding refractions :-D).\n\nHere goes another example with the same IBL but adding some specularity to the material:\n\n<img src=\"uploads/2022/9be65014fb.gif\" width=\"600\" height=\"574\" alt=\"\" />\n\n<img src=\"uploads/2022/c4514647d1.png\" width=\"600\" height=\"600\" alt=\"\" />\n\nOf course this could be pushed further with some SSAO or any other technique that approximates self-shadowing. But this is a very compact and cheap (memory/code/effort) way to display models in an easy-to-author appealing way.\n\nSome final notes:\n\n- If you want to be accurate, the ball should be rendered with an _orthographic_ camera (_i.e.,_ not with a _perspective_ camera).\n- For blurry/glossy or diffuse reflections, you can get away with a low-res ball render. However, make sure that the UVs that you generate don't step on the antialias pixels at the boundary of the rendered ball. Otherwise the model will look b0rked at grazing angles.\n- This technique can be extended by using 2 ball images: one with the diffuse component of the material, and another with the specular component. This way the fragment shader could modulate the diffuse part and the specular part by separate colors that could be passed as inputs to the shader. Keep in mind that this would require that the ball images and the colors are passed in linear (instead of sRGB) space and that the resulting color is sent back to sRGB.\n",
				"date_published": "2022-04-20T04:51:00+02:00",
				"url": "https://www.brashandplucky.com/2022/04/20/fake-viewport-pbr.html",
				"tags": ["render","3d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/04/16/im-currently-swimming.html",
				"title": "Trimmed NURBS",
				"content_html": "<p>I&rsquo;m currently swimming in an ocean of NURBS with their trimming loops. This is experimental work for NURBS rendering in Maverick Render.</p>\n\n<p><img src=\"uploads/2022/b0526fdbcf.png\" width=\"600\" height=\"519\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/162c080550.png\" width=\"600\" height=\"616\" alt=\"\" /></p>\n\n<p><strong>[EDIT]</strong> Throwing some GLSL vert/frag shaders in to the mix for normal/tangency inspection. 2 days in and this is still WIP.</p>\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/6e97cf2bcb.mp4\" width=\"600\" height=\"600\" poster=\"https://www.brashandplucky.com/uploads/2022/2fd92ed526.png\" preload=\"none\"></video>\n",
				"content_text": "I'm currently swimming in an ocean of NURBS with their trimming loops. This is experimental work for NURBS rendering in Maverick Render.\n\n<img src=\"uploads/2022/b0526fdbcf.png\" width=\"600\" height=\"519\" alt=\"\" />\n\n<img src=\"uploads/2022/162c080550.png\" width=\"600\" height=\"616\" alt=\"\" />\n\n**[EDIT]** Throwing some GLSL vert/frag shaders in to the mix for normal/tangency inspection. 2 days in and this is still WIP.\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/6e97cf2bcb.mp4\" width=\"600\" height=\"600\" poster=\"https://www.brashandplucky.com/uploads/2022/2fd92ed526.png\" preload=\"none\"></video>\n",
				"date_published": "2022-04-16T07:44:00+02:00",
				"url": "https://www.brashandplucky.com/2022/04/16/im-currently-swimming.html",
				"tags": ["math","3d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/04/12/a-couple-of.html",
				"title": "Lattice-Boltzmann Method (LBM)",
				"content_html": "<p>A couple of weekends ago I was tinkering with fluid dynamics and implemented a few compute shaders in Unity. Among the ones I implemented, my favorite is the Lattice-Boltzmann Method featured here.</p>\n\n<p><img src=\"uploads/2022/407fc47c9b.jpg\" width=\"575\" height=\"345\" alt=\"\" /></p>\n\n<p>This early implementation allows to draw SDF entities in the simulation space behaving as blockers, inflows or outflows. I intend to expand on the features, but most likely at a slow pace, because this is a no-rush side-project. But I definitely wish to post now and then about my progress.</p>\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/a990f72c82.mp4\" width=\"586\" height=\"356\" poster=\"https://www.brashandplucky.com/uploads/2022/ed97e69515.png\" preload=\"none\"></video>\n\n<p>I also hope to find some spare time soon to write a quick article on how to implement an LBM cellular automata from a light-on-maths perspective.</p>\n",
				"content_text": "A couple of weekends ago I was tinkering with fluid dynamics and implemented a few compute shaders in Unity. Among the ones I implemented, my favorite is the Lattice-Boltzmann Method featured here.\n\n<img src=\"uploads/2022/407fc47c9b.jpg\" width=\"575\" height=\"345\" alt=\"\" />\n\nThis early implementation allows to draw SDF entities in the simulation space behaving as blockers, inflows or outflows. I intend to expand on the features, but most likely at a slow pace, because this is a no-rush side-project. But I definitely wish to post now and then about my progress.\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/a990f72c82.mp4\" width=\"586\" height=\"356\" poster=\"https://www.brashandplucky.com/uploads/2022/ed97e69515.png\" preload=\"none\"></video>\n\nI also hope to find some spare time soon to write a quick article on how to implement an LBM cellular automata from a light-on-maths perspective.\n",
				"date_published": "2022-04-12T23:52:00+02:00",
				"url": "https://www.brashandplucky.com/2022/04/12/a-couple-of.html",
				"tags": ["render","math","2d","physics","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/04/12/224748.html",
				"title": "Colorful pet project",
				"content_html": "<p>Some pet project I am making some progress on. More info soon. Definitely. Maybe.</p>\n\n<p><img src=\"uploads/2022/49bd635e27.jpg\" width=\"600\" height=\"576\" alt=\"\" /></p>\n\n<p><strong>[EDIT] I had to install a micro.blog plug-in so Twitter cards display the main post image when cross-posting to Twitter.</strong> Apologies for the multiple test posts.</p>\n\n<p><strong>[EDIT] Here&rsquo;s a hint.</strong> It&rsquo;s a cutsie CSG-based Solid modeling experiment in Unity Editor.</p>\n\n<p><img src=\"uploads/2022/ce58ca5845.gif\" width=\"600\" height=\"623\" alt=\"\" /></p>\n",
				"content_text": "Some pet project I am making some progress on. More info soon. Definitely. Maybe.\n\n<img src=\"uploads/2022/49bd635e27.jpg\" width=\"600\" height=\"576\" alt=\"\" />\n\n**[EDIT] I had to install a micro.blog plug-in so Twitter cards display the main post image when cross-posting to Twitter.** Apologies for the multiple test posts.\n\n**[EDIT] Here's a hint.** It's a cutsie CSG-based Solid modeling experiment in Unity Editor.\n\n<img src=\"uploads/2022/ce58ca5845.gif\" width=\"600\" height=\"623\" alt=\"\" />\n",
				"date_published": "2022-04-12T22:47:00+02:00",
				"url": "https://www.brashandplucky.com/2022/04/12/224748.html",
				"tags": ["render","math","3d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/02/11/topology-fun.html",
				"title": "Topology fun",
				"content_html": "<p>Recently I have been revisiting in my spare time one of my favorite subjects: <a href=\"https://en.wikipedia.org/wiki/Topology\">topology</a>. I will probably be posting some stuff about topological operators, subdivision surfaces, DCEL structures, irregular tilings and such things in the near future.</p>\n\n<p>Below: A little demo I&rsquo;ve written that supports an extended flavor of <a href=\"https://en.wikipedia.org/wiki/Conway_polyhedron_notation\">Conway&rsquo;s notation</a> for polyhedra: <code>adotaocD</code>.</p>\n\n<p><img src=\"uploads/2022/e9cee91a82.gif\" width=\"533\" height=\"533\" alt=\"\" /></p>\n\n<p><strong>[EDIT] I paused this pet project for the time being&hellip;</strong> because the 2022.2 release of <a href=\"https://maverickrender.com/\">Maverick</a> has virtually blown all my productivity hours away. But I plan to write some entries on topological operators at some point further into this year.</p>\n",
				"content_text": "Recently I have been revisiting in my spare time one of my favorite subjects: [topology](https://en.wikipedia.org/wiki/Topology). I will probably be posting some stuff about topological operators, subdivision surfaces, DCEL structures, irregular tilings and such things in the near future.\n\nBelow: A little demo I've written that supports an extended flavor of [Conway's notation](https://en.wikipedia.org/wiki/Conway_polyhedron_notation) for polyhedra: `adotaocD`.\n\n<img src=\"uploads/2022/e9cee91a82.gif\" width=\"533\" height=\"533\" alt=\"\" />\n\n**[EDIT] I paused this pet project for the time being...** because the 2022.2 release of [Maverick](https://maverickrender.com/) has virtually blown all my productivity hours away. But I plan to write some entries on topological operators at some point further into this year.\n",
				"date_published": "2022-02-12T00:17:00+02:00",
				"url": "https://www.brashandplucky.com/2022/02/11/topology-fun.html",
				"tags": ["math","3d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/30/old-mlt-experiments.html",
				"title": "Old MLT experiments",
				"content_html": "\n\n<p><strong>These are some very old MLT tests that I found on my hard drive today.</strong></p>\n\n<p><a href=\"https://en.wikipedia.org/wiki/Metropolis_light_transport\">Metropolis Light Transport</a> (MLT) is a global illumination variant of the <a href=\"https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\">Metropolis-Hastings</a> algorithm for <a href=\"https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov-Chain Monte Carlo</a> (MCMC).</p>\n\n<p>In a nutshell, MCMC algorithms explore the neighborhood of a Monte Carlo sample by jittering (mutating) said sample. A metric is then used to define <em>how good</em> a sample is, so the next mutation is accepted or rejected based on how good it is compared to the current sample. This leads to a <em>random-walk</em> in the space of samples which tends to crawl up in the increasing direction of the metric used.</p>\n\n<p>This method biases sampling towards successful samples. So a bit of statistics juggling is required to counter that bias. The link below is a seminal paper that beautifully describes the nuts and bolts of this process. The idea presented there is to jitter samples, not in sample space directly, but in quasi-random number tuples space instead:</p>\n\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.4826&amp;rep=rep1&amp;type=pdf\">Simple and Robust Mutation Strategy for Metropolis Light\nTransport Algorithm</a></p>\n\n<p>In the particular case of MLT, samples are random light paths, and the metric used is the amount of light energy transported by a path. This can be implemented on top of regular Path Tracing. Arion was the first (?) commercial render engine that did this on the GPU back in 2012 or so.</p>\n\n<p>MLT has fallen out of favor over the years (path guiding is all the rage these days). But MLT proved to be a competent <em>universal solution</em> to global illumination, being capable of boosting path tracing so it could efficiently solve even the hardest cases, such as refractive caustics.</p>\n\n<p>The beauty of MLT is that it requires 0 configuration from the user and does not need any additional memory for data structures. The main drawbacks are the <em>ugly splotchy and non-uniform look of noise</em>, as compared to regular path tracing, and the inability to guarantee noise <em>stability across animation frames</em>.</p>\n\n<h2 id=\"experiments-in-2d\">Experiments in 2D</h2>\n\n<p>Before I implemented MLT in Arion, I did some MCMC simulations with 2D sampling. In the videos below I used the <code>(x,y)</code> coords of pixels as samples, and the grayscale luminance <code>Y</code> of the image as the metric. The goal of the mutations here is to reconstruct the image by sampling harder in the directions of higher luminance.</p>\n\n<p>MCMC is prone to getting stuck for long amounts of time in local maxima. The practical solution proposed in the above paper is to introduce a <em>plarge</em> probability that kicks the sampling scheme and sends the next sample to a purely random position in the sample space. The videos below visualize this very well I think.</p>\n\n<p><em>Mutating in QRN tuple space without plarge</em> - <a href=\"https://youtu.be/55CXyRqXIJU\">Watch on Youtube</a></p>\n\n<p><img src=\"uploads/2022/51d1741ea4.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p><em>Mutating in QRN tuple space with plarge</em> - <a href=\"https://youtu.be/RJIDema8mz4\">Watch on Youtube</a></p>\n\n<p><img src=\"uploads/2022/4e2da71675.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p><em>MLT reconstructing a grayscale image</em> - <a href=\"https://youtu.be/WYvMGtWp4N4\">Watch on Youtube</a></p>\n\n<p><img src=\"uploads/2022/c8db37186e.png\" width=\"550\" height=\"362\" alt=\"\" /></p>\n\n<p>Implementing MLT successfully in <a href=\"https://maverickrender.com\">our render engine</a> did require some specialization in our sampling routines. I won&rsquo;t get into the details, but basically, since mutations happen in the random tuples that generate the paths, you expect continuous mutations in the tuples to produce continuous changes in the generated paths. So all samplers involved must avoid discontinuities in the <code>[QRN&lt;-&gt;path]</code> bijection.</p>\n\n<h2 id=\"back-in-time\">Back in time</h2>\n\n<p>Some MLT-heavy images rendered in Arion back then:</p>\n\n<p><img src=\"uploads/2022/dc95e60f9e.jpg\" width=\"600\" height=\"429\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/94c3cb35a8.jpg\" width=\"600\" height=\"337\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/45f4967f73.jpg\" width=\"600\" height=\"337\" alt=\"\" /></p>\n",
				"content_text": "**These are some very old MLT tests that I found on my hard drive today.**\n\n[Metropolis Light Transport](https://en.wikipedia.org/wiki/Metropolis_light_transport) (MLT) is a global illumination variant of the [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) algorithm for [Markov-Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC).\n\nIn a nutshell, MCMC algorithms explore the neighborhood of a Monte Carlo sample by jittering (mutating) said sample. A metric is then used to define _how good_ a sample is, so the next mutation is accepted or rejected based on how good it is compared to the current sample. This leads to a _random-walk_ in the space of samples which tends to crawl up in the increasing direction of the metric used.\n\nThis method biases sampling towards successful samples. So a bit of statistics juggling is required to counter that bias. The link below is a seminal paper that beautifully describes the nuts and bolts of this process. The idea presented there is to jitter samples, not in sample space directly, but in quasi-random number tuples space instead:\n\n[Simple and Robust Mutation Strategy for Metropolis Light\nTransport Algorithm](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.4826&rep=rep1&type=pdf)\n\nIn the particular case of MLT, samples are random light paths, and the metric used is the amount of light energy transported by a path. This can be implemented on top of regular Path Tracing. Arion was the first (?) commercial render engine that did this on the GPU back in 2012 or so.\n\nMLT has fallen out of favor over the years (path guiding is all the rage these days). But MLT proved to be a competent _universal solution_ to global illumination, being capable of boosting path tracing so it could efficiently solve even the hardest cases, such as refractive caustics.\n\nThe beauty of MLT is that it requires 0 configuration from the user and does not need any additional memory for data structures. The main drawbacks are the _ugly splotchy and non-uniform look of noise_, as compared to regular path tracing, and the inability to guarantee noise _stability across animation frames_.\n\n## Experiments in 2D\n\nBefore I implemented MLT in Arion, I did some MCMC simulations with 2D sampling. In the videos below I used the `(x,y)` coords of pixels as samples, and the grayscale luminance `Y` of the image as the metric. The goal of the mutations here is to reconstruct the image by sampling harder in the directions of higher luminance.\n\nMCMC is prone to getting stuck for long amounts of time in local maxima. The practical solution proposed in the above paper is to introduce a _plarge_ probability that kicks the sampling scheme and sends the next sample to a purely random position in the sample space. The videos below visualize this very well I think.\n\n_Mutating in QRN tuple space without plarge_ - [Watch on Youtube](https://youtu.be/55CXyRqXIJU)\n\n<img src=\"uploads/2022/51d1741ea4.png\" width=\"512\" height=\"512\" alt=\"\" />\n\n_Mutating in QRN tuple space with plarge_ - [Watch on Youtube](https://youtu.be/RJIDema8mz4)\n\n<img src=\"uploads/2022/4e2da71675.png\" width=\"512\" height=\"512\" alt=\"\" />\n\n_MLT reconstructing a grayscale image_ - [Watch on Youtube](https://youtu.be/WYvMGtWp4N4)\n\n<img src=\"uploads/2022/c8db37186e.png\" width=\"550\" height=\"362\" alt=\"\" />\n\nImplementing MLT successfully in [our render engine](https://maverickrender.com) did require some specialization in our sampling routines. I won't get into the details, but basically, since mutations happen in the random tuples that generate the paths, you expect continuous mutations in the tuples to produce continuous changes in the generated paths. So all samplers involved must avoid discontinuities in the `[QRN<->path]` bijection.\n\n## Back in time\n\nSome MLT-heavy images rendered in Arion back then:\n\n<img src=\"uploads/2022/dc95e60f9e.jpg\" width=\"600\" height=\"429\" alt=\"\" />\n\n<img src=\"uploads/2022/94c3cb35a8.jpg\" width=\"600\" height=\"337\" alt=\"\" />\n\n<img src=\"uploads/2022/45f4967f73.jpg\" width=\"600\" height=\"337\" alt=\"\" />\n",
				"date_published": "2022-01-30T05:52:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/30/old-mlt-experiments.html",
				"tags": ["render","3d","2d"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/uniform-vs-stratified.html",
				"title": "Uniform vs. stratified (2015-03-10)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>This is a classic subject in numerical (Monte Carlo) integration.</p>\n\n<p><em>Uniform 2D distribution vs. Halton series for the first 2 dimensions</em></p>\n\n<p><img src=\"uploads/2022/c5fd50d371.png\" width=\"600\" height=\"300\" alt=\"\" /></p>\n\n<p>To the left: 32768 points in a 512×512 image using a uniform random number generator (Mersenne Twister). To the right, the first 32768 pairs in the Halton series, using dimensions #0 and #1. Click to enlarge!</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\r\n\r\nThis is a classic subject in numerical (Monte Carlo) integration.\r\n\r\n_Uniform 2D distribution vs. Halton series for the first 2 dimensions_\r\n\r\n<img src=\"uploads/2022/c5fd50d371.png\" width=\"600\" height=\"300\" alt=\"\" />\r\n\r\nTo the left: 32768 points in a 512×512 image using a uniform random number generator (Mersenne Twister). To the right, the first 32768 pairs in the Halton series, using dimensions #0 and #1. Click to enlarge!\r\n",
				"date_published": "2022-01-29T16:34:38+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/uniform-vs-stratified.html",
				"tags": ["render","math","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/filling-of-missing.html",
				"title": "Filling of missing image pixels (2015-05-28)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>Here’s what we could call a mean pyramid of the 512×512 <a href=\"https://en.wikipedia.org/wiki/Lena_Fors%C3%A9n\">Lena image</a>. <em>i.e.</em>, a sequence of progressive 1:2 downscalings, where each pixel in the i-th downscaled level is the average of the corresponding 4 pixels in the previous level. For people familiar with OpenGL and such, this is what happens when the mipmaps for a texture map are computed:</p>\n\n<p><em>The 9+1 mipmap levels in the 512×512 Lena image</em></p>\n\n<p><img src=\"uploads/2022/8a609e57da.png\" width=\"512\" height=\"768\" alt=\"\" /></p>\n\n<p>There are many smart, efficient, and relatively simple algorithms based on multi-resolution image pyramids. Modern GPUs can deal with upscaling and downscaling of RGB/A images at the speed of light, so usually such algorithms can be implemented at interactive or even real-time framerates.</p>\n\n<p>Here is the result of upscaling all the mip levels of the Lena image by doing progressive 2:1 bi-linear upscalings until the original resolution is reached:</p>\n\n<p><em>Upscaled mipmap levels</em></p>\n\n<p><img src=\"uploads/2022/8d745486da.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p>Note the characteristic “blocky” (bi-linear) appearance, specially evident in the images of the second row.</p>\n\n<p>Lately, I have been doing some experimentation with <strong>pixel extrapolation</strong> algorithms that “restore” the missing pixels in an incomplete image for a 2D DOF filter based on the Depth AOV.</p>\n\n<p>My pixel extrapolation algorithm works in 2 stages:</p>\n\n<ol>\n<li>The first stage (analysis) prepares the mean pyramid of the source image by doing progressive 1:2 downscalings. Only the meaningful (not-a-hole) pixels in each 2x2 packet are averaged down. If a 2x2 packet does not have any meaningful pixels, a hole is passed to the next (lower) level.</li>\n<li>The second stage (synthesis) starts at the smallest level and goes up, leaving meaningful pixels intact, while replacing holes by upscaled data from the previous (lower) level.</li>\n</ol>\n\n<p><em>Mean pyramid (with holes)</em></p>\n\n<p><img src=\"uploads/2022/ab15c77d06.png\" width=\"512\" height=\"768\" alt=\"\" /></p>\n\n<p>Note that the analysis stage can stop as soon as a mip level doesn’t have any holes.</p>\n\n<p>Here is the full algorithm, successfully filling the missing pixels in the image.</p>\n\n<p><em>Mean pyramid (filled)</em></p>\n\n<p><img src=\"uploads/2022/92cab1f783.png\" width=\"512\" height=\"768\" alt=\"\" /></p>\n\n<h2 id=\"conclusions\">Conclusions:</h2>\n\n<p>This algorithm can be implemented in an extremely efficient fashion on the GPU, and allows for fantastic parallelization on the CPU as well. The locality of color/intensity is preserved reasonably well, although holes become smears of color “too easily”.</p>\n\n<p>A small (classic) inconvenience is that the source image must be pre-padded to a power-of-2 size for the progressive 1:2 downscalings to be well defined. I picked an image that is a power-of-2 in size already in the above examples.</p>\n\n<p>TL;DR: Given the ease of implementation and the extreme potential in terms of efficiency, the results are quite decent for many applications.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nHere’s what we could call a mean pyramid of the 512×512 [Lena image](https://en.wikipedia.org/wiki/Lena_Fors%C3%A9n). _i.e._, a sequence of progressive 1:2 downscalings, where each pixel in the i-th downscaled level is the average of the corresponding 4 pixels in the previous level. For people familiar with OpenGL and such, this is what happens when the mipmaps for a texture map are computed:\n\n_The 9+1 mipmap levels in the 512×512 Lena image_\n\n<img src=\"uploads/2022/8a609e57da.png\" width=\"512\" height=\"768\" alt=\"\" />\n\nThere are many smart, efficient, and relatively simple algorithms based on multi-resolution image pyramids. Modern GPUs can deal with upscaling and downscaling of RGB/A images at the speed of light, so usually such algorithms can be implemented at interactive or even real-time framerates.\n\nHere is the result of upscaling all the mip levels of the Lena image by doing progressive 2:1 bi-linear upscalings until the original resolution is reached:\n\n_Upscaled mipmap levels_\n\n<img src=\"uploads/2022/8d745486da.png\" width=\"600\" height=\"600\" alt=\"\" />\n\nNote the characteristic “blocky” (bi-linear) appearance, specially evident in the images of the second row.\n\nLately, I have been doing some experimentation with **pixel extrapolation** algorithms that “restore” the missing pixels in an incomplete image for a 2D DOF filter based on the Depth AOV.\n\nMy pixel extrapolation algorithm works in 2 stages:\n\n1. The first stage (analysis) prepares the mean pyramid of the source image by doing progressive 1:2 downscalings. Only the meaningful (not-a-hole) pixels in each 2x2 packet are averaged down. If a 2x2 packet does not have any meaningful pixels, a hole is passed to the next (lower) level.\n2. The second stage (synthesis) starts at the smallest level and goes up, leaving meaningful pixels intact, while replacing holes by upscaled data from the previous (lower) level.\n\n_Mean pyramid (with holes)_\n\n<img src=\"uploads/2022/ab15c77d06.png\" width=\"512\" height=\"768\" alt=\"\" />\n\nNote that the analysis stage can stop as soon as a mip level doesn’t have any holes.\n\nHere is the full algorithm, successfully filling the missing pixels in the image.\n\n_Mean pyramid (filled)_\n\n<img src=\"uploads/2022/92cab1f783.png\" width=\"512\" height=\"768\" alt=\"\" />\n\n## Conclusions:\n\nThis algorithm can be implemented in an extremely efficient fashion on the GPU, and allows for fantastic parallelization on the CPU as well. The locality of color/intensity is preserved reasonably well, although holes become smears of color “too easily”.\n\nA small (classic) inconvenience is that the source image must be pre-padded to a power-of-2 size for the progressive 1:2 downscalings to be well defined. I picked an image that is a power-of-2 in size already in the above examples.\n\nTL;DR: Given the ease of implementation and the extreme potential in terms of efficiency, the results are quite decent for many applications.\n",
				"date_published": "2022-01-29T16:01:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/filling-of-missing.html",
				"tags": ["2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/playing-with-the.html",
				"title": "Playing with the Fourier Transform (2016-07-28)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>The beauty and power of the <a href=\"https://en.wikipedia.org/wiki/Fourier_transform\">Fourier Transform</a> never cease to amaze me. And since several effects in <a href=\"http://www.arionfx.com\">ArionFX</a> are based on it, I have had to play with it a lot in recent times.</p>\n\n<p>As explained in a previous post, diffraction patterns (<em>e.g.,</em> the glare simulated in ArionFX for Photoshop) come from the Fourier Transform of the lens aperture. I will use the FT of an aperture mask for visualization in this post.</p>\n\n<p>I will use pow-2 square sizes (my FT implementation is an FFT). Let’s start by this Aperture x Obstacle map output directly from ArionFX for Photoshop v3.5.0.</p>\n\n<p><em>Aperture x Obstacle mask, rasterized @ 512×512</em></p>\n\n<p><img src=\"uploads/2022/0923ef842a.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p>The Fourier Transform of that image, rasterized at that size in said 512×512 buffer, is the following.</p>\n\n<p><em>FT of the mask above</em></p>\n\n<p><img src=\"uploads/2022/91b6b3b6d3.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p>The faux-color is done on the magnitude of each complex number in the FT. All the FT images in this post are normalized equally, and offset to look “centered” around the mid-pixel.</p>\n\n<p>Such diffraction patterns and some heavy convolution magic are what ArionFX uses to compute glare on HDR images:</p>\n\n<p><em>Resulting glare in ArionFX</em></p>\n\n<p><img src=\"uploads/2022/012e902dbf.png\" width=\"480\" height=\"320\" alt=\"\" /></p>\n\n<p>&ndash;</p>\n\n<p>Now, let’s focus on what happens to the FT (frequency space) when one does certain operations on the source data (image space). Or, in this exemplification: what happens to the diffraction pattern, when one plays with the rasterized aperture mask.</p>\n\n<p>Note that we’re speaking of the Discrete Fourier Transform, so sampling (rasterization, pixelization) issues are mostly ignored.</p>\n\n<h3 id=\"rotation-about-the-center\">Rotation about the center</h3>\n\n<p>A rotation of the source buffer about its center doesn’t change the frequencies present in the data; only their orientation. So a rotation in the source data rotates the FT rotates in the exact same way.</p>\n\n<p>As we will see next, this property holds true regardless of the center of rotation, because the FT is invariant with respect to translations.</p>\n\n<p><em>Rotation about the center</em></p>\n\n<p><img src=\"uploads/2022/a31a55c21f.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<h3 id=\"translation-with-warp-around\">Translation (with warp-around)</h3>\n\n<p>Frequencies arise from the relative position of values in the data field, and not from their absolute position in said field. For this reason, shifting (warp-around included) the source data does not affect the corresponding Fourier Transform in any way.</p>\n\n<p><em>Invariance to translation</em></p>\n\n<p><img src=\"uploads/2022/09db31ab29.png\" width=\"600\" height=\"900\" alt=\"\" /></p>\n\n<p>Let’s recall that the idea behind the FT is that <em>“any periodic function can be rewritten as a weighted sum of sines and cosines of different frequencies”</em>. Periodic being the keyword there.</p>\n\n<h3 id=\"repetition-tiling\">Repetition (tiling)</h3>\n\n<p>Tiling the data buffer NxM times (e.g., 2×2 in the example below) produces the same FT, but with frequencies “exploded” every NxM cells, canceling out everywhere else.</p>\n\n<p>This is because no new frequencies are introduced, since we are transforming the same source data. However, the source data is NxM times smaller proportional to the data buffer size (i.e., the frequencies become NxM times higher).</p>\n\n<p><em>Exploded frequencies on tiling</em></p>\n\n<p><img src=\"uploads/2022/6ed11eb3b1.png\" width=\"600\" height=\"300\" alt=\"\" /></p>\n\n<h3 id=\"data-scaling\">Data scaling</h3>\n\n<p>Normalization and sampling issues aside, scaling the data within the source buffer scales the FT inversely.</p>\n\n<p>This is because encoding smaller data requires higher frequencies, while encoding a larger version of the same data requires lower frequencies.</p>\n\n<p><em>Inverse effect on scaling</em></p>\n\n<p><img src=\"uploads/2022/c7adf5446a.png\" width=\"600\" height=\"1200\" alt=\"\" /></p>\n\n<p>In the particular case of glare (e.g., ArionFX) this means that the diffraction pattern becomes blurry if the iris is sampled small. Or, in other words, for a given iris, the sharpest diffraction pattern possible is achieved when the iris is sampled as large as the data buffer itself.</p>\n\n<p>Note, however, that “large” here means “with respect to the data buffer”, being the size of the data buffer irrelevant as we will see next.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nThe beauty and power of the [Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform) never cease to amaze me. And since several effects in [ArionFX](http://www.arionfx.com) are based on it, I have had to play with it a lot in recent times.\n\nAs explained in a previous post, diffraction patterns (_e.g.,_ the glare simulated in ArionFX for Photoshop) come from the Fourier Transform of the lens aperture. I will use the FT of an aperture mask for visualization in this post.\n\nI will use pow-2 square sizes (my FT implementation is an FFT). Let’s start by this Aperture x Obstacle map output directly from ArionFX for Photoshop v3.5.0.\n\n_Aperture x Obstacle mask, rasterized @ 512×512_\n\n<img src=\"uploads/2022/0923ef842a.png\" width=\"512\" height=\"512\" alt=\"\" />\n\nThe Fourier Transform of that image, rasterized at that size in said 512×512 buffer, is the following.\n\n_FT of the mask above_\n\n<img src=\"uploads/2022/91b6b3b6d3.png\" width=\"512\" height=\"512\" alt=\"\" />\n\nThe faux-color is done on the magnitude of each complex number in the FT. All the FT images in this post are normalized equally, and offset to look “centered” around the mid-pixel.\n\nSuch diffraction patterns and some heavy convolution magic are what ArionFX uses to compute glare on HDR images:\n\n_Resulting glare in ArionFX_\n\n<img src=\"uploads/2022/012e902dbf.png\" width=\"480\" height=\"320\" alt=\"\" />\n\n--\n\nNow, let’s focus on what happens to the FT (frequency space) when one does certain operations on the source data (image space). Or, in this exemplification: what happens to the diffraction pattern, when one plays with the rasterized aperture mask.\n\nNote that we’re speaking of the Discrete Fourier Transform, so sampling (rasterization, pixelization) issues are mostly ignored.\n\n### Rotation about the center\n\nA rotation of the source buffer about its center doesn’t change the frequencies present in the data; only their orientation. So a rotation in the source data rotates the FT rotates in the exact same way.\n\nAs we will see next, this property holds true regardless of the center of rotation, because the FT is invariant with respect to translations.\n\n_Rotation about the center_\n\n<img src=\"uploads/2022/a31a55c21f.png\" width=\"600\" height=\"600\" alt=\"\" />\n\n### Translation (with warp-around)\n\nFrequencies arise from the relative position of values in the data field, and not from their absolute position in said field. For this reason, shifting (warp-around included) the source data does not affect the corresponding Fourier Transform in any way.\n\n_Invariance to translation_\n\n<img src=\"uploads/2022/09db31ab29.png\" width=\"600\" height=\"900\" alt=\"\" />\n\nLet’s recall that the idea behind the FT is that _“any periodic function can be rewritten as a weighted sum of sines and cosines of different frequencies”_. Periodic being the keyword there.\n\n### Repetition (tiling)\n\nTiling the data buffer NxM times (e.g., 2×2 in the example below) produces the same FT, but with frequencies “exploded” every NxM cells, canceling out everywhere else.\n\nThis is because no new frequencies are introduced, since we are transforming the same source data. However, the source data is NxM times smaller proportional to the data buffer size (i.e., the frequencies become NxM times higher).\n\n_Exploded frequencies on tiling_\n\n<img src=\"uploads/2022/6ed11eb3b1.png\" width=\"600\" height=\"300\" alt=\"\" />\n\n### Data scaling\n\nNormalization and sampling issues aside, scaling the data within the source buffer scales the FT inversely.\n\nThis is because encoding smaller data requires higher frequencies, while encoding a larger version of the same data requires lower frequencies.\n\n_Inverse effect on scaling_\n\n<img src=\"uploads/2022/c7adf5446a.png\" width=\"600\" height=\"1200\" alt=\"\" />\n\nIn the particular case of glare (e.g., ArionFX) this means that the diffraction pattern becomes blurry if the iris is sampled small. Or, in other words, for a given iris, the sharpest diffraction pattern possible is achieved when the iris is sampled as large as the data buffer itself.\n\nNote, however, that “large” here means “with respect to the data buffer”, being the size of the data buffer irrelevant as we will see next.\n",
				"date_published": "2022-01-29T16:01:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/playing-with-the.html",
				"tags": ["render","math","2d","physics","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/hosek-wilkie-sky.html",
				"title": "Hosek \u0026 Wilkie sky model (2015-03-03)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>This is an old comparison between the Preetham and Hosek &amp; Wilkie sky/sun models.</p>\n\n<p><em>Preetham et al. sky model</em></p>\n\n<p><img src=\"uploads/2022/6c9c260561.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p><em>Hosek &amp; Wilkie sky model</em></p>\n\n<p><img src=\"uploads/2022/70e29da8dd.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nThis is an old comparison between the Preetham and Hosek & Wilkie sky/sun models.\n\n_Preetham et al. sky model_\n\n<img src=\"uploads/2022/6c9c260561.png\" width=\"600\" height=\"600\" alt=\"\" />\n\n_Hosek & Wilkie sky model_\n\n<img src=\"uploads/2022/70e29da8dd.png\" width=\"600\" height=\"600\" alt=\"\" />\n",
				"date_published": "2022-01-29T16:00:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/hosek-wilkie-sky.html",
				"tags": ["render","3d","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/scrambled-halton.html",
				"title": "Scrambled Halton (2015-03-10)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>The Halton sequence, which is one of my favourite algorithms ever, can be used for efficient stratified multi-dimensional sampling. Some references:</p>\n\n<ul>\n<li><a href=\"https://en.wikipedia.org/wiki/Van_der_Corput_sequence\">Van der Corput sequence</a></li>\n<li><a href=\"https://en.wikipedia.org/wiki/Halton_sequence\">Halton sequence</a></li>\n</ul>\n\n<p>It is possible to do stratified sampling of hyper-points in the <em>s-dimensional</em> unit hyper-cube by picking one consecutive dimension of the Halton series for each component. A convenient way to do so is to use the first <em>s</em> prime numbers as the basis for each Halton sequence.</p>\n\n<p>It is well-known, however, that while this approach works great for low dimensions, high dimensions often exhibit a great degree of undesired correlation. The following image displays a grid where each cell combines two components of the <em>32-dimensional</em> Halton hyper-cube.</p>\n\n<p><em>Raw 32-dimensional Halton hyper-cube</em></p>\n\n<p><img src=\"uploads/2022/cac06392ac.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p>One can easily spot how some pairs exhibit an obvious pattern, while others fill their corresponding 2D area very densely. This happens more aggressively for higher dimensions (<em>i.e.,</em> to the right/bottom in the image) and for pairs formed with close components (<em>i.e.,</em> near the diagonal in the image). Click to enlarge!</p>\n\n<p>A successful approach to dissolve this problem without losing the good properties of stratification is to do <em>“random digit scrambling”</em> (<em>a.k.a.,</em> rds). During the construction of a Halton number, digits in the range <code>[0..base[</code> are combined. Given a <em>Pseudo-Random Permutation</em> of <code>length=base</code>, all that one must do is use <code>PRP(digit)</code> instead of digit directly. This somewhat shuffles Halton pairs in rows and columns in a strong way so the correlation disappears. However, since the PRP is a bijection, the good properties of stratification are generally preserved.</p>\n\n<p>How to build a strong and efficient randomised PRP of an arbitrary length is an interesting subject which details I won’t get into here.</p>\n\n<p>Here’s the scrambling strategy in action:</p>\n\n<p><em>Scrambled 32-dimensional Halton hyper-cube</em></p>\n\n<p><img src=\"uploads/2022/2df80353ac.png\" width=\"600\" height=\"600\" alt=\"\" /></p>\n\n<p>Now all the blocks in the grid look equally dense. Magic!</p>\n\n<p>As long as one picks good PRPs, it is possible to generate any number of different samplings, all with the same good properties.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nThe Halton sequence, which is one of my favourite algorithms ever, can be used for efficient stratified multi-dimensional sampling. Some references:\n\n- [Van der Corput sequence](https://en.wikipedia.org/wiki/Van_der_Corput_sequence)\n- [Halton sequence](https://en.wikipedia.org/wiki/Halton_sequence)\n\nIt is possible to do stratified sampling of hyper-points in the _s-dimensional_ unit hyper-cube by picking one consecutive dimension of the Halton series for each component. A convenient way to do so is to use the first _s_ prime numbers as the basis for each Halton sequence.\n\nIt is well-known, however, that while this approach works great for low dimensions, high dimensions often exhibit a great degree of undesired correlation. The following image displays a grid where each cell combines two components of the _32-dimensional_ Halton hyper-cube.\n\n_Raw 32-dimensional Halton hyper-cube_\n\n<img src=\"uploads/2022/cac06392ac.png\" width=\"600\" height=\"600\" alt=\"\" />\n\nOne can easily spot how some pairs exhibit an obvious pattern, while others fill their corresponding 2D area very densely. This happens more aggressively for higher dimensions (_i.e.,_ to the right/bottom in the image) and for pairs formed with close components (_i.e.,_ near the diagonal in the image). Click to enlarge!\n\nA successful approach to dissolve this problem without losing the good properties of stratification is to do _“random digit scrambling”_ (_a.k.a.,_ rds). During the construction of a Halton number, digits in the range `[0..base[` are combined. Given a _Pseudo-Random Permutation_ of `length=base`, all that one must do is use `PRP(digit)` instead of digit directly. This somewhat shuffles Halton pairs in rows and columns in a strong way so the correlation disappears. However, since the PRP is a bijection, the good properties of stratification are generally preserved.\n\nHow to build a strong and efficient randomised PRP of an arbitrary length is an interesting subject which details I won’t get into here.\n\nHere’s the scrambling strategy in action:\n\n_Scrambled 32-dimensional Halton hyper-cube_\n\n<img src=\"uploads/2022/2df80353ac.png\" width=\"600\" height=\"600\" alt=\"\" />\n\nNow all the blocks in the grid look equally dense. Magic!\n\nAs long as one picks good PRPs, it is possible to generate any number of different samplings, all with the same good properties.\n",
				"date_published": "2022-01-29T16:00:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/scrambled-halton.html",
				"tags": ["render","math","3d"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/sobel-operator.html",
				"title": "Sobel operator (2014-09-07)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>The <a href=\"https://en.wikipedia.org/wiki/Sobel_operator\">Sobel operator</a> is a simple way to approximate the gradient of the intensity in an image. This, in visual terms, can be used for <em>edge detection</em>. The purpose of edge detection is to significantly reduce the amount of data in the image, while preserving the structural properties to be used for further image processing.</p>\n\n<p>In practice, the Sobel operator is simply a pair of 3×3 (separable) <em>convolution kernels</em>. One highlights the horizontal gradient/edges, and the other one highlights the vertical gradient/edges.</p>\n\n<p><img src=\"uploads/2022/628b3579aa.png\" width=\"160\" height=\"60\" alt=\"\" /></p>\n\n<p><img src=\"uploads/2022/cf022b06bf.png\" width=\"160\" height=\"60\" alt=\"\" /></p>\n\n<p>In non-formal terms, and under certain theoretical assumptions, this is conceptually equivalent to computing the partial derivatives of the image with respect to x an y.</p>\n\n<p>For the examples below, I am using as input the same image featured by Wikipedia in the Sobel operator page:</p>\n\n<p><em>Sobel operator</em></p>\n\n<p><img src=\"uploads/2022/b2d2de2218.png\" width=\"600\" height=\"681\" alt=\"\" /></p>\n\n<p>This grid presents:</p>\n\n<ol>\n<li>The input image (luminance).</li>\n<li>The absolute magnitude of the result of the Sobel filter.</li>\n<li>The result of the Sobel (x) filter.</li>\n<li>The result of the Sobel (y) filter.</li>\n<li>Same as (2), but in faux color.</li>\n<li>The gradient vectors, normalized and displayed as an RGB-encoded normal map.</li>\n</ol>\n\n<p>The 3-tap Sobel convolution kernels have a 1px radius, so they have a very limited edge detection range. This makes the filter quite shaky as soon as the image presents fine details or noise. For this reason, one may want to pre-pass the input image with a subtle Gaussian blur.</p>\n\n<p>This has the effect of diminishing edges in general (as expected), but the resulting gradient images are equivalent, yet much cleaner.</p>\n\n<p>Sobel operator (Gaussian with sigma=2)</p>\n\n<p><img src=\"uploads/2022/cce8b82fd4.png\" width=\"600\" height=\"681\" alt=\"\" /></p>\n\n<p>The Sobel operator is one of the most fundamental building blocks in feature detection in the field of Computer Vision.</p>\n\n<p>Note that the Sobel operator does not characterize edges or detects features in any way. It simply produces a filtered image where pixels that most likely belong to an area of high gradient (such as an edge) are highlighted.</p>\n\n<p><em>Bonus remark:</em> Sobel filtering is very similar to what a render engine such as <a href=\"https://maverickrender.com\">Maverick</a> does to transform a height map (bump map) into a normal map.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nThe [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator) is a simple way to approximate the gradient of the intensity in an image. This, in visual terms, can be used for _edge detection_. The purpose of edge detection is to significantly reduce the amount of data in the image, while preserving the structural properties to be used for further image processing.\n\nIn practice, the Sobel operator is simply a pair of 3×3 (separable) _convolution kernels_. One highlights the horizontal gradient/edges, and the other one highlights the vertical gradient/edges.\n\n<img src=\"uploads/2022/628b3579aa.png\" width=\"160\" height=\"60\" alt=\"\" />\n\n<img src=\"uploads/2022/cf022b06bf.png\" width=\"160\" height=\"60\" alt=\"\" />\n\nIn non-formal terms, and under certain theoretical assumptions, this is conceptually equivalent to computing the partial derivatives of the image with respect to x an y.\n\nFor the examples below, I am using as input the same image featured by Wikipedia in the Sobel operator page:\n\n_Sobel operator_\n\n<img src=\"uploads/2022/b2d2de2218.png\" width=\"600\" height=\"681\" alt=\"\" />\n\nThis grid presents:\n\n1. The input image (luminance).\n2. The absolute magnitude of the result of the Sobel filter.\n3. The result of the Sobel (x) filter.\n4. The result of the Sobel (y) filter.\n5. Same as (2), but in faux color.\n6. The gradient vectors, normalized and displayed as an RGB-encoded normal map.\n\nThe 3-tap Sobel convolution kernels have a 1px radius, so they have a very limited edge detection range. This makes the filter quite shaky as soon as the image presents fine details or noise. For this reason, one may want to pre-pass the input image with a subtle Gaussian blur.\n\nThis has the effect of diminishing edges in general (as expected), but the resulting gradient images are equivalent, yet much cleaner.\n\nSobel operator (Gaussian with sigma=2)\n\n<img src=\"uploads/2022/cce8b82fd4.png\" width=\"600\" height=\"681\" alt=\"\" />\n\nThe Sobel operator is one of the most fundamental building blocks in feature detection in the field of Computer Vision.\n\nNote that the Sobel operator does not characterize edges or detects features in any way. It simply produces a filtered image where pixels that most likely belong to an area of high gradient (such as an edge) are highlighted.\n\n_Bonus remark:_ Sobel filtering is very similar to what a render engine such as [Maverick](https://maverickrender.com) does to transform a height map (bump map) into a normal map.\n",
				"date_published": "2022-01-29T15:56:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/sobel-operator.html",
				"tags": ["math","2d"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/the-error-function.html",
				"title": "The Error function (erf) (2014-09-06)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>Here is the 1D Gaussian function:</p>\n\n<p><img src=\"uploads/2022/0a79850a48.png\" width=\"136\" height=\"31\" alt=\"\" /></p>\n\n<p>Put in short, the <a href=\"https://en.wikipedia.org/wiki/Error_function\">Error function</a> is the integral of the Gaussian function from 0 to a certain point x:</p>\n\n<p><img src=\"uploads/2022/545dcc77e5.png\" width=\"139\" height=\"22\" alt=\"\" /></p>\n\n<p>At least, that is the way the formula is presented by Wikipedia and <a href=\"https://mathworld.wolfram.com/Erf.html\">Wolfram|Alpha</a>. But as soon as you try to work with it you find out that in order to really match the above Gaussian function, normalization and axis scaling must be taken care of:</p>\n\n<p><img src=\"uploads/2022/353905b96a.png\" width=\"159\" height=\"27\" alt=\"\" /></p>\n\n<p>The plots below display <code>G(x,sigma)</code> (bell-shaped) in blue, and <code>erf(x,sigma)</code> (S-shaped) in yellow.</p>\n\n<p>A very typical use for <code>G(x,sigma)</code> that I’ve been talking about on this blog lately, is to build convolution kernels for Gaussian blur. An image convolution kernel, for example, is a pixel-centered discretization of a certain underlying function (a Gaussian, in this case). Said discretization splits the x axis in uniform unit-length bins (<em>a.k.a.,</em> taps, or intervals) centered at x=0.</p>\n\n<p>For each bin, this is pretty much the definition of the integral of <code>G(x,sigma)</code> along the bin. That is, the increment of <code>erf(x,sigma)</code> between both end-points of the bin.</p>\n\n<p><em>Discretized 1D Gaussian (sigma=0.5)</em></p>\n\n<p><img src=\"uploads/2022/7753576e75.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p><em>Discretized 1D Gaussian (sigma=1.0)</em></p>\n\n<p><img src=\"uploads/2022/4bef51783b.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<p><em>Discretized 1D Gaussian (sigma=1.5)</em></p>\n\n<p><img src=\"uploads/2022/6352091d75.png\" width=\"512\" height=\"512\" alt=\"\" /></p>\n\n<h3 id=\"doing-it-wrong\">Doing it wrong:</h3>\n\n<p>Most implementations of Gaussian convolution kernels simply evaluate <code>G(x,sigma)</code> at the mid-point of each bin. This is a decent approximation that is also trivial to implement:</p>\n\n<p><img src=\"uploads/2022/fdd4889b6e.png\" width=\"162\" height=\"20\" alt=\"\" /></p>\n\n<p>This value is represented in blue in the above plots.</p>\n\n<h3 id=\"implementing-erf\">Implementing erf:</h3>\n\n<p>While <code>G(x,sigma)</code> has a trivial explicit formulation, <code>erf</code> is an archetypical non-elementary function.</p>\n\n<p>Some development environments provide an <code>erf</code> implementation. But most (the ones I use) don’t. There are some smart example implementations out there if you Google a bit. Usually they are either numerical approximations, or piece-wise interpolations.</p>\n\n<p>Assuming that one has an <code>erf</code> implementation at his disposal, the correct value for the bin <code>[a..b]</code> is:</p>\n\n<p><img src=\"uploads/2022/f461644e4a.png\" width=\"232\" height=\"17\" alt=\"\" /></p>\n\n<p>This value is represented in red in the above plots.</p>\n\n<h3 id=\"a-poor-man-s-approximation\">A poor-man&rsquo;s approximation:</h3>\n\n<p>If you are feeling lazy, an alternative approximation is to super-sample <code>G(x,sigma)</code> along the bin. The amount of samples can be chosen to be proportional to how small <code>sigma</code> is.</p>\n\n<p>This method is easy to implement. However, it is far less elegant and also less efficient as more evaluation calls are required.</p>\n\n<h3 id=\"conclusion\">Conclusion:</h3>\n\n<p>When discretizing a Gaussian function, as sigma becomes small (close to or smaller than <code>(b-a)</code>), the approximation <code>bin(a,b)</code> (in blue above) deviates significantly from the correct <code>bin'(a,b)</code> (in red above).</p>\n\n<p>So if you are going to Gaussian-blur at radius 1px or smaller, and precision is important, then it is necessary to use <code>erf(x,sigma)</code> or at least super-sample <code>G(x,sigma)</code>.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nHere is the 1D Gaussian function:\n\n<img src=\"uploads/2022/0a79850a48.png\" width=\"136\" height=\"31\" alt=\"\" />\n\nPut in short, the [Error function](https://en.wikipedia.org/wiki/Error_function) is the integral of the Gaussian function from 0 to a certain point x:\n\n<img src=\"uploads/2022/545dcc77e5.png\" width=\"139\" height=\"22\" alt=\"\" />\n\nAt least, that is the way the formula is presented by Wikipedia and [Wolfram|Alpha](https://mathworld.wolfram.com/Erf.html). But as soon as you try to work with it you find out that in order to really match the above Gaussian function, normalization and axis scaling must be taken care of:\n\n<img src=\"uploads/2022/353905b96a.png\" width=\"159\" height=\"27\" alt=\"\" />\n\nThe plots below display `G(x,sigma)` (bell-shaped) in blue, and `erf(x,sigma)` (S-shaped) in yellow.\n\nA very typical use for `G(x,sigma)` that I’ve been talking about on this blog lately, is to build convolution kernels for Gaussian blur. An image convolution kernel, for example, is a pixel-centered discretization of a certain underlying function (a Gaussian, in this case). Said discretization splits the x axis in uniform unit-length bins (_a.k.a.,_ taps, or intervals) centered at x=0.\n\nFor each bin, this is pretty much the definition of the integral of `G(x,sigma)` along the bin. That is, the increment of `erf(x,sigma)` between both end-points of the bin.\n\n_Discretized 1D Gaussian (sigma=0.5)_\n\n<img src=\"uploads/2022/7753576e75.png\" width=\"512\" height=\"512\" alt=\"\" />\n\n_Discretized 1D Gaussian (sigma=1.0)_\n\n<img src=\"uploads/2022/4bef51783b.png\" width=\"512\" height=\"512\" alt=\"\" />\n\n_Discretized 1D Gaussian (sigma=1.5)_\n\n<img src=\"uploads/2022/6352091d75.png\" width=\"512\" height=\"512\" alt=\"\" />\n\n### Doing it wrong:\n\nMost implementations of Gaussian convolution kernels simply evaluate `G(x,sigma)` at the mid-point of each bin. This is a decent approximation that is also trivial to implement:\n\n<img src=\"uploads/2022/fdd4889b6e.png\" width=\"162\" height=\"20\" alt=\"\" />\n\nThis value is represented in blue in the above plots.\n\n### Implementing erf:\n\nWhile `G(x,sigma)` has a trivial explicit formulation, `erf` is an archetypical non-elementary function.\n\nSome development environments provide an `erf` implementation. But most (the ones I use) don’t. There are some smart example implementations out there if you Google a bit. Usually they are either numerical approximations, or piece-wise interpolations.\n\nAssuming that one has an `erf` implementation at his disposal, the correct value for the bin `[a..b]` is:\n\n<img src=\"uploads/2022/f461644e4a.png\" width=\"232\" height=\"17\" alt=\"\" />\n\nThis value is represented in red in the above plots.\n\n### A poor-man's approximation:\n\nIf you are feeling lazy, an alternative approximation is to super-sample `G(x,sigma)` along the bin. The amount of samples can be chosen to be proportional to how small `sigma` is.\n\nThis method is easy to implement. However, it is far less elegant and also less efficient as more evaluation calls are required.\n\n### Conclusion:\n\nWhen discretizing a Gaussian function, as sigma becomes small (close to or smaller than `(b-a)`), the approximation `bin(a,b)` (in blue above) deviates significantly from the correct `bin'(a,b)` (in red above).\n\nSo if you are going to Gaussian-blur at radius 1px or smaller, and precision is important, then it is necessary to use `erf(x,sigma)` or at least super-sample `G(x,sigma)`.\n",
				"date_published": "2022-01-29T15:55:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/the-error-function.html",
				"tags": ["math","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/downsampling-and-gaussian.html",
				"title": "Downsampling and Gaussian blur (2014-09-01)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>I talked about several strategies to optimize convolutions in some of my previous posts. I still got to talk about how to approximate a Gaussian blur using a multi-step Box blur in a future post. However, there is yet another good technique to optimize a Gaussian blur that may come handy in some cases.</p>\n\n<p>This post is inspired by a need that I had some days ago: Say that you need to do a 3D Gaussian blur on a potentially humongous 3D data buffer. Working with downsampled data sounds ideal in terms of storage and performance. So that’s what I am going to talk about here:</p>\n\n<p><em>What happens when downsampled data is used as input for a Gaussian blur?</em></p>\n\n<h2 id=\"the-idea\">The idea:</h2>\n\n<p>Here’s the 0-centered and un-normalized 1D Gaussian function:</p>\n\n<p><img src=\"uploads/2022/0b597462a0.png\" width=\"103\" height=\"26\" alt=\"\" /></p>\n\n<p>The <em>sigma</em> parameter in the Gaussian function stretches the bell shape along the x axis. So it is quite straightforward to understand that if one downsamples the input dataset by a scale factor <em>k</em>, then applies a (smaller) Gaussian where <em>sigma’=s/k</em>, and finally upscales the result by the same scale factor <em>k</em>, the result will approximate a true Gaussian on the original dataset where sigma=s.</p>\n\n<p>In cleaner terms: if one has an input dataset (<em>e.g.,</em> an image) <em>I</em> and wants to have it blurred by a Gaussian where <em>sigma=s</em>:</p>\n\n<ol>\n<li><em>I’&lt;=I</em> downsampled by a certain scale factor <em>k</em>.</li>\n<li><em>I”&lt;=I&rsquo;</em> blurred by a small Gaussian where <em>s’=s/k</em>.</li>\n<li><em>I”’&lt;=I&rdquo;</em> upscaled by a scale factor <em>k</em>.</li>\n</ol>\n\n<h2 id=\"how-good-is-this-approximation\">How good is this approximation?</h2>\n\n<p>The <a href=\"https://mathworld.wolfram.com/SamplingTheorem.html\">Sampling Theorem</a> states that sampling a signal at (at least) twice its smallest wavelength is enough. Which means that downsampling cuts frequencies above the <em>Nyquist</em> limit (half the sampling rate). In other words: Downsampling means less data to process, but at the expense of introducing an error.</p>\n\n<p>Fortunately, a Gaussian blur is a form of low-pass frequency filter. This means that blurring is quite tolerant to alterations in the high part of the frequency spectrum.</p>\n\n<h2 id=\"visual-evaluation\">Visual evaluation:</h2>\n\n<p>In the examples below I am downsampling with a simple pixel average, and I am upscaling with a simple bilinear filter. The 2x2 grids below compare:</p>\n\n<ol>\n<li>Top-left – The original image <em>I</em>.</li>\n<li>Top-right – <em>I</em> downsampled and upscaled by k (note the blocky bilinear filter look).</li>\n<li>Bottom-left – The resulting image <em>I”’</em>.</li>\n<li>Bottom-right – <em>I</em> blurred by a true Gaussian where sigma=s.</li>\n</ol>\n\n<p>In these examples, <em>k=sigma</em> was chosen for simplicity. This means that the small Gaussian uses <em>sigma’=1</em>.</p>\n\n<p><em>Gaussian blur where sigma=4</em></p>\n\n<p><img src=\"uploads/2022/3fc179f5ba.png\" width=\"600\" height=\"401\" alt=\"\" /></p>\n\n<p><em>Gaussian blur where sigma=16</em></p>\n\n<p><img src=\"uploads/2022/c9d9705c2e.png\" width=\"600\" height=\"401\" alt=\"\" /></p>\n\n<p><em>Gaussian blur where sigma=64</em></p>\n\n<p><img src=\"uploads/2022/87d800496a.png\" width=\"600\" height=\"401\" alt=\"\" /></p>\n\n<h2 id=\"conclusion\">Conclusion:</h2>\n\n<p>As shown, the approximation (bottom-left vs. bottom-right) is pretty good.</p>\n\n<p>The gain in speed depends on multiple implementation factors. However, as I explained above, this post was inspired by a need to cope with a cubic memory storage problem when doing Gaussian blurs on a 3D buffer. Working with a heavily downsampled buffer clearly helps in that sense. And it is needless to say that decreasing the amount of data to process by <em>k^3</em> also brings a dramatic speed boost, making it possible to use tiny separable convolutions along the 3 (downsampled) axes.</p>\n\n<p>Note that one might pick any downsampling scale factor <em>k&gt;=1</em>. The higher the value of <em>k</em>, the higher the approximation error and the smaller and faster the convolution.</p>\n\n<p>The choice <em>k=sigma</em> offers a good trade-off between approximation error and speed gain as shown above.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nI talked about several strategies to optimize convolutions in some of my previous posts. I still got to talk about how to approximate a Gaussian blur using a multi-step Box blur in a future post. However, there is yet another good technique to optimize a Gaussian blur that may come handy in some cases.\n\nThis post is inspired by a need that I had some days ago: Say that you need to do a 3D Gaussian blur on a potentially humongous 3D data buffer. Working with downsampled data sounds ideal in terms of storage and performance. So that’s what I am going to talk about here:\n\n_What happens when downsampled data is used as input for a Gaussian blur?_\n\n## The idea:\n\nHere’s the 0-centered and un-normalized 1D Gaussian function:\n\n<img src=\"uploads/2022/0b597462a0.png\" width=\"103\" height=\"26\" alt=\"\" />\n\nThe _sigma_ parameter in the Gaussian function stretches the bell shape along the x axis. So it is quite straightforward to understand that if one downsamples the input dataset by a scale factor _k_, then applies a (smaller) Gaussian where _sigma’=s/k_, and finally upscales the result by the same scale factor _k_, the result will approximate a true Gaussian on the original dataset where sigma=s.\n\nIn cleaner terms: if one has an input dataset (_e.g.,_ an image) _I_ and wants to have it blurred by a Gaussian where _sigma=s_:\n\n1. _I’<=I_ downsampled by a certain scale factor _k_.\n2. _I”<=I'_ blurred by a small Gaussian where _s’=s/k_.\n3. _I”’<=I''_ upscaled by a scale factor _k_.\n\n## How good is this approximation?\n\nThe [Sampling Theorem](https://mathworld.wolfram.com/SamplingTheorem.html) states that sampling a signal at (at least) twice its smallest wavelength is enough. Which means that downsampling cuts frequencies above the _Nyquist_ limit (half the sampling rate). In other words: Downsampling means less data to process, but at the expense of introducing an error.\n\nFortunately, a Gaussian blur is a form of low-pass frequency filter. This means that blurring is quite tolerant to alterations in the high part of the frequency spectrum.\n\n## Visual evaluation:\n\nIn the examples below I am downsampling with a simple pixel average, and I am upscaling with a simple bilinear filter. The 2x2 grids below compare:\n\n1. Top-left – The original image _I_.\n2. Top-right – _I_ downsampled and upscaled by k (note the blocky bilinear filter look).\n3. Bottom-left – The resulting image _I”’_.\n4. Bottom-right – _I_ blurred by a true Gaussian where sigma=s.\n\nIn these examples, _k=sigma_ was chosen for simplicity. This means that the small Gaussian uses _sigma’=1_.\n\n_Gaussian blur where sigma=4_\n\n<img src=\"uploads/2022/3fc179f5ba.png\" width=\"600\" height=\"401\" alt=\"\" />\n\n_Gaussian blur where sigma=16_\n\n<img src=\"uploads/2022/c9d9705c2e.png\" width=\"600\" height=\"401\" alt=\"\" />\n\n_Gaussian blur where sigma=64_\n\n<img src=\"uploads/2022/87d800496a.png\" width=\"600\" height=\"401\" alt=\"\" />\n\n## Conclusion:\n\nAs shown, the approximation (bottom-left vs. bottom-right) is pretty good.\n\nThe gain in speed depends on multiple implementation factors. However, as I explained above, this post was inspired by a need to cope with a cubic memory storage problem when doing Gaussian blurs on a 3D buffer. Working with a heavily downsampled buffer clearly helps in that sense. And it is needless to say that decreasing the amount of data to process by _k^3_ also brings a dramatic speed boost, making it possible to use tiny separable convolutions along the 3 (downsampled) axes.\n\nNote that one might pick any downsampling scale factor _k>=1_. The higher the value of _k_, the higher the approximation error and the smaller and faster the convolution.\n\nThe choice _k=sigma_ offers a good trade-off between approximation error and speed gain as shown above.\n",
				"date_published": "2022-01-29T15:54:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/downsampling-and-gaussian.html",
				"tags": ["math","2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/diaphragm-and-fstop.html",
				"title": "Diaphragm and f-stop (2014-08-17)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>This is just another image taken from the Maverick Unit Testing system. The chart displays different polygonal diaphragms at different f-stop values. Doubling the f-stop number, halves the surface light can pass through.</p>\n\n<p><em>Polygonal diaphragms and f-stop</em></p>\n\n<p><img src=\"uploads/2022/f558614798.png\" width=\"544\" height=\"512\" alt=\"\" /></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\r\n\r\nThis is just another image taken from the Maverick Unit Testing system. The chart displays different polygonal diaphragms at different f-stop values. Doubling the f-stop number, halves the surface light can pass through.\r\n\r\n_Polygonal diaphragms and f-stop_\r\n\r\n\n\n<img src=\"uploads/2022/f558614798.png\" width=\"544\" height=\"512\" alt=\"\" />\n",
				"date_published": "2022-01-29T15:51:10+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/diaphragm-and-fstop.html",
				"tags": ["math","2d","physics","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/glare-patterns.html",
				"title": "Glare patterns (2014-08-14)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>Glare in photography is due to <a href=\"https://en.wikipedia.org/wiki/Fraunhofer_diffraction\">Fraunhofer diffraction</a> as light from distant objects passes through the camera diaphragm.</p>\n\n<p>There is a magical connection between Fraunhofer diffraction (physics) and the Fourier Transform (math). As a matter of fact, the intensity of the Fraunhofer diffraction pattern of a certain aperture is given by the squared modulus of the Fourier Transform of said aperture.</p>\n\n<p>Assuming a clean and unobstacled camera, the aperture is the diaphragm shape. Here you have the diffraction patterns that correspond to some basic straight-blade (polygonal) diaphragms.</p>\n\n<p><em>Glare patterns</em></p>\n\n<p><img src=\"uploads/2022/d174a5ac84.png\" width=\"600\" height=\"225\" alt=\"\" /></p>\n\n<p>Interestingly, the Fourier Transform produces one infinite decaying streak perpendicular to each polygon edge. When the number of edges is even, the streaks overlap in pairs. That is why an hexagonal diaphragm produces 6 streaks, and an heptagonal diaphragm produces 14.</p>\n\n<p>The leftmost pattern happens to be the <a href=\"https://en.wikipedia.org/wiki/Airy_disk\">Airy disk</a>. The Airy disk is a limit case where the number of polygon edges/streaks is infinite.</p>\n\n<p>The examples above were generated at 256x256. The visual definition of the pattern naturally depends on the resolution of the buffers involved in the computation of the Fourier Transform. However, note that the FT has an infinite range. This means that for ideal polygonal shapes, the streaks are infinitely long.</p>\n\n<p>In the practical case, buffers are far from infinite, and you hit one property of the Fourier Transform that is often nothing but an annoyance: the FT is cyclic. The image below depicts what happens when one pumps up the intensity of one of the glare patterns obtained above: the (infinite) streaks, which warp-around the (finite) FT buffer, become evident.</p>\n\n<p><em>Cyclic glare pattern</em></p>\n\n<p><img src=\"uploads/2022/070390a6aa.png\" width=\"600\" height=\"285\" alt=\"\" /></p>\n\n<p><em>Bonus:</em> Here’s some real-life glare I screengrabbed this evening at the European Athletics Championships.</p>\n\n<p><em>Real-life glare</em></p>\n\n<p><img src=\"uploads/2022/475a218b49.png\" width=\"512\" height=\"300\" alt=\"\" /></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nGlare in photography is due to [Fraunhofer diffraction](https://en.wikipedia.org/wiki/Fraunhofer_diffraction) as light from distant objects passes through the camera diaphragm.\n\nThere is a magical connection between Fraunhofer diffraction (physics) and the Fourier Transform (math). As a matter of fact, the intensity of the Fraunhofer diffraction pattern of a certain aperture is given by the squared modulus of the Fourier Transform of said aperture.\n\nAssuming a clean and unobstacled camera, the aperture is the diaphragm shape. Here you have the diffraction patterns that correspond to some basic straight-blade (polygonal) diaphragms.\n\n_Glare patterns_\n\n<img src=\"uploads/2022/d174a5ac84.png\" width=\"600\" height=\"225\" alt=\"\" />\n\nInterestingly, the Fourier Transform produces one infinite decaying streak perpendicular to each polygon edge. When the number of edges is even, the streaks overlap in pairs. That is why an hexagonal diaphragm produces 6 streaks, and an heptagonal diaphragm produces 14.\n\nThe leftmost pattern happens to be the [Airy disk](https://en.wikipedia.org/wiki/Airy_disk). The Airy disk is a limit case where the number of polygon edges/streaks is infinite.\n\nThe examples above were generated at 256x256. The visual definition of the pattern naturally depends on the resolution of the buffers involved in the computation of the Fourier Transform. However, note that the FT has an infinite range. This means that for ideal polygonal shapes, the streaks are infinitely long.\n\nIn the practical case, buffers are far from infinite, and you hit one property of the Fourier Transform that is often nothing but an annoyance: the FT is cyclic. The image below depicts what happens when one pumps up the intensity of one of the glare patterns obtained above: the (infinite) streaks, which warp-around the (finite) FT buffer, become evident.\n\n_Cyclic glare pattern_\n\n<img src=\"uploads/2022/070390a6aa.png\" width=\"600\" height=\"285\" alt=\"\" />\n\n_Bonus:_ Here’s some real-life glare I screengrabbed this evening at the European Athletics Championships.\n\n_Real-life glare_\n\n<img src=\"uploads/2022/475a218b49.png\" width=\"512\" height=\"300\" alt=\"\" />\n",
				"date_published": "2022-01-29T15:49:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/glare-patterns.html",
				"tags": ["math","2d","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/my-latest-running.html",
				"title": "My latest running shoes (2014-08-07)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>I have run my latest 7000 km or so on Nike Lunaracer+ (v1 and v3) shoes. This week I started my last pair. I will have to re-stock soon.</p>\n\n<p><em>My latest 7000 km</em></p>\n\n<p><img src=\"uploads/2022/bce75368a2.jpg\" width=\"600\" height=\"384\" alt=\"\" /></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nI have run my latest 7000 km or so on Nike Lunaracer+ (v1 and v3) shoes. This week I started my last pair. I will have to re-stock soon.\n\n_My latest 7000 km_\n\n\n\n<img src=\"uploads/2022/bce75368a2.jpg\" width=\"600\" height=\"384\" alt=\"\" />\n",
				"date_published": "2022-01-29T15:47:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/my-latest-running.html",
				"tags": ["personal"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/fast-convolutions-iii.html",
				"title": "Fast convolutions (III) (2014-07-19)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>Some more remarks about the performance of the convolution methods described so far. I will be leaving the brute-force algorithm out for obvious reasons.</p>\n\n<p>The plot below represents input size (in pixels) in the x axis, and convolution time (in seconds) in the y axis. So, if you go to <em>x=4096</em> that means <em>“a convolution of a 4096×4096 image by a 4096×4096 kernel”</em>.</p>\n\n<p><em>Even competition between FFT-based vs. separable convolution methods</em></p>\n\n<p><img src=\"uploads/2022/e0fc16f520.png\" width=\"516\" height=\"516\" alt=\"\" /></p>\n\n<p>Two conclusions can be made from the above plot, which confirm what was explained in my previous post:</p>\n\n<ol>\n<li>For large kernels (as large as the image itself) the separable convolution method is <em>O(n^3)</em> and times get to absurd levels very quickly. If you are dealing with large generic images/kernels, the FFT-based method is the way to go.</li>\n<li>The FFT-based method uses the Fast Fourier Transform, which is <em>O(n^2·log(n))</em> thanks to some decomposition technique that requires the size of the input data to be (padded to) a power-of-2. For this reason, it takes the same amount of time to do a convolution on a <em>257×257</em> image/kernel than on a <em>512×512</em> image/kernel, because both cases operate on <em>512×512</em> buffers after all. This is why the graph for the FFT method is stepped. When x crosses a power-of-2, the running time goes up all of a sudden and stays stable until the next power-of-2.</li>\n</ol>\n\n<p>The plot was generated with my current implementation of both convolution methods in MK_api. My FFT uses the Cooley-Tukey algorithm, and everything (i.e., FFT, IFFT, point-wise products, and 1D separable convolutions) makes use of multi-threading. There’s always room for improvement, but the running times seem pretty decent, as we’re speaking of <em>&lt;2s</em> for images up to <em>4096×4096</em> in a 16-thread CPU. An implementation in CUDA would be (orders of magnitude) faster, though. 😛</p>\n\n<p><strong>[EDIT] It&rsquo;s been 8 years since this post (2014..2022). Now we use the cuFFT implementation in Maverick, which is blazingly fast.</strong></p>\n\n<p><em>Separable convolution with a fixed 11×11 kernel (in orange)</em></p>\n\n<p><img src=\"uploads/2022/e8ec462e51.png\" width=\"516\" height=\"516\" alt=\"\" /></p>\n\n<p>A couple more remarks:</p>\n\n<ol>\n<li>The graph of the FFT-based method wouldn’t change if smaller kernels were used, as the algorithm requires the kernel to be padded to the size of the image. However, the graph of the separable method becomes much less steep when very small kernels are used:</li>\n<li>The running time of the FFT/IFFT is not exactly a trivial subject. Speed in FFT/IFFT algorithms depends not only on the size of the data, but also on the data itself. While generating the above plots, I came across some anomalies in the time measurements. Some kernels or some images produced faster or slower results. Some combinations would even produce non-flat steps in the staircase-looking graph. That’s normal, but worth mentioning.</li>\n</ol>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nSome more remarks about the performance of the convolution methods described so far. I will be leaving the brute-force algorithm out for obvious reasons.\n\nThe plot below represents input size (in pixels) in the x axis, and convolution time (in seconds) in the y axis. So, if you go to _x=4096_ that means _“a convolution of a 4096×4096 image by a 4096×4096 kernel”_.\n\n_Even competition between FFT-based vs. separable convolution methods_\n\n<img src=\"uploads/2022/e0fc16f520.png\" width=\"516\" height=\"516\" alt=\"\" />\n\nTwo conclusions can be made from the above plot, which confirm what was explained in my previous post:\n\n1. For large kernels (as large as the image itself) the separable convolution method is _O(n^3)_ and times get to absurd levels very quickly. If you are dealing with large generic images/kernels, the FFT-based method is the way to go.\n2. The FFT-based method uses the Fast Fourier Transform, which is _O(n^2·log(n))_ thanks to some decomposition technique that requires the size of the input data to be (padded to) a power-of-2. For this reason, it takes the same amount of time to do a convolution on a _257×257_ image/kernel than on a _512×512_ image/kernel, because both cases operate on _512×512_ buffers after all. This is why the graph for the FFT method is stepped. When x crosses a power-of-2, the running time goes up all of a sudden and stays stable until the next power-of-2.\n\nThe plot was generated with my current implementation of both convolution methods in MK_api. My FFT uses the Cooley-Tukey algorithm, and everything (i.e., FFT, IFFT, point-wise products, and 1D separable convolutions) makes use of multi-threading. There’s always room for improvement, but the running times seem pretty decent, as we’re speaking of _<2s_ for images up to _4096×4096_ in a 16-thread CPU. An implementation in CUDA would be (orders of magnitude) faster, though. 😛\n\n**[EDIT] It's been 8 years since this post (2014..2022). Now we use the cuFFT implementation in Maverick, which is blazingly fast.**\n\n_Separable convolution with a fixed 11×11 kernel (in orange)_\n\n<img src=\"uploads/2022/e8ec462e51.png\" width=\"516\" height=\"516\" alt=\"\" />\n\nA couple more remarks:\n\n1. The graph of the FFT-based method wouldn’t change if smaller kernels were used, as the algorithm requires the kernel to be padded to the size of the image. However, the graph of the separable method becomes much less steep when very small kernels are used:\n2. The running time of the FFT/IFFT is not exactly a trivial subject. Speed in FFT/IFFT algorithms depends not only on the size of the data, but also on the data itself. While generating the above plots, I came across some anomalies in the time measurements. Some kernels or some images produced faster or slower results. Some combinations would even produce non-flat steps in the staircase-looking graph. That’s normal, but worth mentioning.\n",
				"date_published": "2022-01-29T15:44:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/fast-convolutions-iii.html",
				"tags": ["math","2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/29/fast-convolutions-ii.html",
				"title": "Fast convolutions (II) (2014-07-18)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>I will analyze the algorithmic complexity of the convolution algorithms described in my previous posts.</p>\n\n<p>To make things simpler, let’s assume that the dimensions of the image are &gt;= the dimensions of the convolution kernel, and that both are square, with dimensions <em>S·S</em> and <em>s·s</em>, respectively.</p>\n\n<h2 id=\"naive-algorithm-o-n-4\">Naive algorithm - O(n^4)</h2>\n\n<p><em>&ldquo;wxh operations for each of the W·H image pixels”.</em></p>\n\n<p><em>i.e.,</em> <em>S·S·s·s</em> operations. This is quadratic with a heavy constant for tiny kernels, but quickly becomes quartic for medium-to-large kernels.</p>\n\n<p>The auxiliary buffer can be identical in size and bit-depth to the original image. So the memory usage factor is 2x.</p>\n\n<h2 id=\"separable-convolution-o-n-3\">Separable convolution - O(n^3)</h2>\n\n<p><em>&ldquo;One 1D convolution for each row + One 1D convolution for each column”.</em></p>\n\n<p><em>i.e.,</em> <em>2·S·S·s</em> operations. This is quadratic with a bad constant for small kernels, but becomes cubic for large kernels.</p>\n\n<p>Remarkably, the total running time depends on the dimensions of the image -and- the dimensions of the kernel.</p>\n\n<p>Again, the auxiliary buffer can be identical in size and bit-depth to the original image. So the memory usage factor is 2x.</p>\n\n<h2 id=\"convolution-theorem-o-n-2-log-n\">Convolution theorem - O(n^2·log(n))</h2>\n\n<p><em>“Two FFTs + One point-wise product + One IFFT”.</em></p>\n\n<p>Let’s call <em>S’</em> to the closest power-of-2 such that <em>S’&gt;=S</em>. Then a proper implementation of the FFT/IFFT does (approx.) <em>2·S’·S’·log(S`)</em> operations, while the point-wise product does <em>S’·S’</em> operations. This makes the algorithm <em>O(S’·S’·log(S’))</em> with some heavy (quadratic) overhead due to the memory copying, padding, and the point-wise product.</p>\n\n<p>Remarkably, the total running time is independent of the size of the kernel.</p>\n\n<p>This algorithm is quite memory hungry, though, because two <em>S’·S’</em> complex-number buffers are required. This means two floating-point numbers per entry (i.e., the real/imaginary coefficients). The algorithm starts by copying the image/kernel to the real part of the corresponding complex-number buffer, leaving the imaginary coefficient and the surface excess filled with zeroes. Then the FFTs/product/IFFT happen in-place.</p>\n\n<p>So the auxiliary memory required is <em>4·S’·S’</em> floating-point numbers.</p>\n\n<p>In the worst case where <em>S</em> is a power-of-2-plus-1, <em>S’</em> gets nearly twice as large as <em>S</em>. If the original image is 8-bit and we are using single-precision floating-point math for the FFT/IFFT, this means a memory usage factor of 64x. In the case of an HDR (single-precision floating-point) grayscale image, we are speaking of a worst case scenario of 16x. In average, however, the memory usage factor is around 8x. If <em>S</em> is a power-of-2, then the memory usage factor goes down to 4x.</p>\n\n<p><em>Heavy glare using the FFT-based method in an HDR image by Paul Debevec</em></p>\n\n<p><img src=\"uploads/2022/b55d5885bf.gif\" width=\"341\" height=\"512\" alt=\"\" /></p>\n\n<p>This image with heavy glare has been output with some ArionFX-related experimental tonemapping code I am working on these days.</p>\n\n<h3 id=\"conclusions\">Conclusions:</h3>\n\n<p>Assuming that we are only interested in sheer performance:</p>\n\n<ol>\n<li>The FFT-based method is (by far) the fastest for large images/kernels. Interestingly, the algorithm is not affected by the size of the kernel, which can be as large as the (padded) image itself without a penalty.</li>\n<li>The FFT-based method becomes even faster if the same kernel is applied multiple times. The kernel FFT can be calculated just once, and then be re-used.</li>\n<li>Due to the heavy setup overhead in the FFT-based method, the separable method can be faster for small (separable) kernels where <em>s</em> is in the range of <em>log(S’)</em>.</li>\n</ol>\n\n<p>Last, but certainly not least, <strong>there is a much faster and more light-weight algorithm for the special case of Box/Gaussian Blur</strong>. I will talk about this in a separate blog entry.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nI will analyze the algorithmic complexity of the convolution algorithms described in my previous posts.\n\nTo make things simpler, let’s assume that the dimensions of the image are >= the dimensions of the convolution kernel, and that both are square, with dimensions _S·S_ and _s·s_, respectively.\n\n## Naive algorithm - O(n^4)\n\n_\"wxh operations for each of the W·H image pixels”._\n\n_i.e.,_ _S·S·s·s_ operations. This is quadratic with a heavy constant for tiny kernels, but quickly becomes quartic for medium-to-large kernels.\n\nThe auxiliary buffer can be identical in size and bit-depth to the original image. So the memory usage factor is 2x.\n\n## Separable convolution - O(n^3)\n\n_\"One 1D convolution for each row + One 1D convolution for each column”._\n\n_i.e.,_ _2·S·S·s_ operations. This is quadratic with a bad constant for small kernels, but becomes cubic for large kernels.\n\nRemarkably, the total running time depends on the dimensions of the image -and- the dimensions of the kernel.\n\nAgain, the auxiliary buffer can be identical in size and bit-depth to the original image. So the memory usage factor is 2x.\n\n## Convolution theorem - O(n^2·log(n))\n\n_“Two FFTs + One point-wise product + One IFFT”._\n\nLet’s call _S’_ to the closest power-of-2 such that _S’>=S_. Then a proper implementation of the FFT/IFFT does (approx.) _2·S’·S’·log(S`)_ operations, while the point-wise product does _S’·S’_ operations. This makes the algorithm _O(S’·S’·log(S’))_ with some heavy (quadratic) overhead due to the memory copying, padding, and the point-wise product.\n\nRemarkably, the total running time is independent of the size of the kernel.\n\nThis algorithm is quite memory hungry, though, because two _S’·S’_ complex-number buffers are required. This means two floating-point numbers per entry (i.e., the real/imaginary coefficients). The algorithm starts by copying the image/kernel to the real part of the corresponding complex-number buffer, leaving the imaginary coefficient and the surface excess filled with zeroes. Then the FFTs/product/IFFT happen in-place.\n\nSo the auxiliary memory required is _4·S’·S’_ floating-point numbers.\n\nIn the worst case where _S_ is a power-of-2-plus-1, _S’_ gets nearly twice as large as _S_. If the original image is 8-bit and we are using single-precision floating-point math for the FFT/IFFT, this means a memory usage factor of 64x. In the case of an HDR (single-precision floating-point) grayscale image, we are speaking of a worst case scenario of 16x. In average, however, the memory usage factor is around 8x. If _S_ is a power-of-2, then the memory usage factor goes down to 4x.\n\n_Heavy glare using the FFT-based method in an HDR image by Paul Debevec_\n\n<img src=\"uploads/2022/b55d5885bf.gif\" width=\"341\" height=\"512\" alt=\"\" />\n\nThis image with heavy glare has been output with some ArionFX-related experimental tonemapping code I am working on these days.\n\n### Conclusions:\n\nAssuming that we are only interested in sheer performance:\n\n1. The FFT-based method is (by far) the fastest for large images/kernels. Interestingly, the algorithm is not affected by the size of the kernel, which can be as large as the (padded) image itself without a penalty.\n2. The FFT-based method becomes even faster if the same kernel is applied multiple times. The kernel FFT can be calculated just once, and then be re-used.\n3. Due to the heavy setup overhead in the FFT-based method, the separable method can be faster for small (separable) kernels where _s_ is in the range of _log(S’)_.\n\nLast, but certainly not least, **there is a much faster and more light-weight algorithm for the special case of Box/Gaussian Blur**. I will talk about this in a separate blog entry.\n",
				"date_published": "2022-01-29T15:43:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/29/fast-convolutions-ii.html",
				"tags": ["math","2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/28/fast-convolutions-i.html",
				"title": "Fast convolutions (I) (2014-07-18)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011…</strong></p>\n\n<p>In my <a href=\"https://www.brashandplucky.com/pointspread-functions-convolutions.html\">previous post</a> it was stated that the convolution of a WxH image with a wxh kernel is a new WxH image where each pixel is the sum of wxh products obtained as the central pixel of the kernel slides across each of the pixels in the original image. This double-double loop leads to an impractical <code>O(n^4)</code> algorithm complexity.</p>\n\n<p>Fortunately, we can do better, but the key here is not in optimizing the code, but in making use of some mathematical weaponry. Let’s analyze the options that we have:</p>\n\n<ol>\n<li>Naive implementation.</li>\n<li>Separable convolution kernels.</li>\n<li>The convolution theorem.</li>\n<li>Can we do EVEN better?</li>\n</ol>\n\n<h2 id=\"naive-implementation\">Naive implementation.</h2>\n\n<p><em>Pros:</em></p>\n\n<ul>\n<li>Trivial implementation in just a few lines of code.</li>\n<li>Works for any input size, and for any type of kernel.</li>\n<li>Trivial clamping at the boundaries.</li>\n<li>Allows for multi-threading.</li>\n</ul>\n\n<p><em>Cons:</em></p>\n\n<ul>\n<li>Embarrassingly inefficient: <code>O(n^4)</code>.</li>\n<li>Requires an auxiliary WxH buffer.</li>\n<li>By default, the output of the operation is returned in the auxiliary buffer (not <em>in-place</em>).</li>\n<li>Not very cache friendly due to the constant jumps across rows.\n– Applying the same kernel multiple times has the same cost, every time.</li>\n</ul>\n\n<p><em>When should I use this method?</em></p>\n\n<ul>\n<li>Never, unless the input data is tiny and clean code is more important than sheer performance.</li>\n</ul>\n\n<h2 id=\"separable-convolution-kernels\">Separable convolution kernels.</h2>\n\n<p>A separable convolution kernel is one that can be broken into two 1D (vertical and horizontal) projections. For these projections the (matrix) product of the 1xh vertical kernel by the wx1 horizontal kernel must restore the original wxh 2D kernel.</p>\n\n<p><em>1D vertical and horizontal Gaussian convolution</em></p>\n\n<p><img src=\"uploads/2022/4171094912.png\" width=\"600\" height=\"402\" alt=\"\" /></p>\n\n<p>Conveniently enough, the most usual convolution kernels (<em>e.g.,</em> Gaussian blur, box blur, …) happen to be separable.</p>\n\n<p>The convolution of an image by a separable convolution kernel becomes the following:</p>\n\n<ol>\n<li>Convolute the rows of the original image with the horizontal kernel projection.</li>\n<li>Convolute the columns of the resulting image with the vertical kernel projection.</li>\n</ol>\n\n<p>Note: These two steps are commutative.</p>\n\n<p><em>2-step separable vs. brute-force 2D Gaussian convolution</em></p>\n\n<p><img src=\"uploads/2022/35a6c714ba.png\" width=\"600\" height=\"608\" alt=\"\" /></p>\n\n<p><em>Pros:</em></p>\n\n<ul>\n<li>More efficient than the naive implementation: <code>O(n^3)</code>.</li>\n<li>Trivial implementation in just a few lines of code.</li>\n<li>Trivial clamping at the boundaries.</li>\n<li>Works for any input size.</li>\n<li>Since this is a two-step process, the convolution can be returned <em>in-place</em>.</li>\n<li>Cache-friendly.</li>\n<li>Allows for multi-threading.</li>\n</ul>\n\n<p><em>Cons:</em></p>\n\n<ul>\n<li>Only works with separable kernels.</li>\n<li>Needs an auxiliary WxH buffer.</li>\n<li>Applying the same kernel multiple times has the same cost, every time.</li>\n</ul>\n\n<p><em>When should I use this method?</em></p>\n\n<ul>\n<li>We do not use this method anywhere in Maverick&rsquo;s API. You will understand why soon.</li>\n</ul>\n\n<h2 id=\"the-convolution-theorem\">The convolution theorem.</h2>\n\n<p>“The <a href=\"https://en.wikipedia.org/wiki/Convolution_theorem\">convolution</a> in the spatial domain is equivalent to a point-wise product in the frequency domain, and vice-versa.”</p>\n\n<p>This method relies on the (Fast) <a href=\"https://en.wikipedia.org/wiki/Fourier_transform\">Fourier Transform</a>, which is one of the most beautiful mathematical constructs, ever. Seriously!</p>\n\n<p>The convolution of an image by a generic kernel becomes the following:</p>\n\n<ol>\n<li>Compute the Fourier Transform of the image.</li>\n<li>Compute the Fourier Transform of the kernel.</li>\n<li>Multiply both Fourier Transforms, point-wise.</li>\n<li>Compute the Inverse Fourier Transform of the result.</li>\n</ol>\n\n<p><em>Brute-force 2D Gaussian vs. the convolution theorem</em></p>\n\n<p><img src=\"uploads/2022/71f4742163.png\" width=\"600\" height=\"609\" alt=\"\" /></p>\n\n<p><em>Magic</em></p>\n\n<p><img src=\"uploads/2022/9fd4b3d82e.gif\" width=\"350\" height=\"196\" alt=\"\" /></p>\n\n<p><em>Pros:</em></p>\n\n<ul>\n<li>Even more efficient: <code>O(n^2·log(n))</code>.</li>\n<li>Works with any convolution kernel, separable or not.</li>\n<li>Should the kernel be applied multiple times, the FFT of the kernel can be computed just once, and then be re-used.</li>\n<li>The FFT/IFFT and the convolution product are cache-friendly.</li>\n<li>The FFT/IFFT and the convolution product allow for multi-threading.</li>\n</ul>\n\n<p><em>Cons:</em></p>\n\n<ul>\n<li>Definitely not easy to implement, unless you already own an FFT module that suits your needs.</li>\n<li>The FFT operates on buffers with power-of-2 dimensions. This means that the input image (and the kernel) must be padded with zeroes to a larger size (i.e., extra setup time + memory).</li>\n<li>Both the image and the kernel are transformed, which requires two auxiliary buffers instead of one.</li>\n<li>Each FFT produces complex-number values, which doubles the memory usage of each auxiliary buffer.</li>\n<li>The dynamic range of the FFT values is generally wilder than that of the original image. This requires a careful implementation, and the use of floating-point math regardless of the bit-depth and range of the input image.</li>\n<li>This method doesn’t do any clamping at the boundaries of the image, producing a very annoying warp-around effect that may need special handling (e.g., more padding).</li>\n</ul>\n\n<p><em>When should I use this method?</em></p>\n\n<ul>\n<li>Every time that a large generic convolution kernel (i.e., not a simple blur) is involved.</li>\n<li>The most obvious examples I can think of in Maverick&rsquo;s are Glare &amp; Bloom.</li>\n</ul>\n\n<h2 id=\"can-we-do-even-better\">Can we do EVEN better?</h2>\n\n<p>Oh yes. We can do much better, at least in the very particular case of blur. I will talk about this in detail in a dedicated blog entry, at some point.</p>\n\n<p><strong>Some implementation remarks:</strong></p>\n\n<p>All the algorithms presented above allow for <strong>multi-threading</strong>. Naturally, MT does not change the algorithm complexity, since the maximum number of threads is fixed, but you can get very decent speeds in practical cases if you combine sleek code with proper multi-threading. In the separable case (2), MT must be used twice. First for the rows, and then for the cols. In the FFT case (3), the FFT itself can be multi-threaded (in a rows/cols fashion as well). The huge point-wise product in frequency space can be MT’ed too.</p>\n\n<p>Since convolutions are usually applied on very large images, writing <strong>cache-friendly code</strong> can make a big difference. Assuming that the memory layout of your image/kernel is per rows, make sure to arrange your loops so memory accesses are as consecutive as possible. This is immediate for the loops that do a 1D convolution on each row. However, for the loops that do a 1D convolution on each column, it may help to use a local cache to transpose a column to a row back and forth.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011…**\n\nIn my [previous post](https://www.brashandplucky.com/pointspread-functions-convolutions.html) it was stated that the convolution of a WxH image with a wxh kernel is a new WxH image where each pixel is the sum of wxh products obtained as the central pixel of the kernel slides across each of the pixels in the original image. This double-double loop leads to an impractical `O(n^4)` algorithm complexity.\n\nFortunately, we can do better, but the key here is not in optimizing the code, but in making use of some mathematical weaponry. Let’s analyze the options that we have:\n\n1. Naive implementation.\n2. Separable convolution kernels.\n3. The convolution theorem.\n4. Can we do EVEN better?\n\n## Naive implementation.\n\n_Pros:_\n\n- Trivial implementation in just a few lines of code.\n- Works for any input size, and for any type of kernel.\n- Trivial clamping at the boundaries.\n- Allows for multi-threading.\n\n_Cons:_\n\n- Embarrassingly inefficient: `O(n^4)`.\n- Requires an auxiliary WxH buffer.\n- By default, the output of the operation is returned in the auxiliary buffer (not _in-place_).\n- Not very cache friendly due to the constant jumps across rows.\n– Applying the same kernel multiple times has the same cost, every time.\n\n_When should I use this method?_\n\n- Never, unless the input data is tiny and clean code is more important than sheer performance.\n\n## Separable convolution kernels.\n\nA separable convolution kernel is one that can be broken into two 1D (vertical and horizontal) projections. For these projections the (matrix) product of the 1xh vertical kernel by the wx1 horizontal kernel must restore the original wxh 2D kernel.\n\n_1D vertical and horizontal Gaussian convolution_\n\n<img src=\"uploads/2022/4171094912.png\" width=\"600\" height=\"402\" alt=\"\" />\n\nConveniently enough, the most usual convolution kernels (_e.g.,_ Gaussian blur, box blur, …) happen to be separable.\n\nThe convolution of an image by a separable convolution kernel becomes the following:\n\n1. Convolute the rows of the original image with the horizontal kernel projection.\n2. Convolute the columns of the resulting image with the vertical kernel projection.\n\nNote: These two steps are commutative.\n\n_2-step separable vs. brute-force 2D Gaussian convolution_\n\n<img src=\"uploads/2022/35a6c714ba.png\" width=\"600\" height=\"608\" alt=\"\" />\n\n_Pros:_\n\n- More efficient than the naive implementation: `O(n^3)`.\n- Trivial implementation in just a few lines of code.\n- Trivial clamping at the boundaries.\n- Works for any input size.\n- Since this is a two-step process, the convolution can be returned _in-place_.\n- Cache-friendly.\n- Allows for multi-threading.\n\n_Cons:_\n\n- Only works with separable kernels.\n- Needs an auxiliary WxH buffer.\n- Applying the same kernel multiple times has the same cost, every time.\n\n_When should I use this method?_\n\n- We do not use this method anywhere in Maverick's API. You will understand why soon.\n\n## The convolution theorem.\n\n“The [convolution](https://en.wikipedia.org/wiki/Convolution_theorem) in the spatial domain is equivalent to a point-wise product in the frequency domain, and vice-versa.”\n\nThis method relies on the (Fast) [Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform), which is one of the most beautiful mathematical constructs, ever. Seriously!\n\nThe convolution of an image by a generic kernel becomes the following:\n\n1. Compute the Fourier Transform of the image.\n2. Compute the Fourier Transform of the kernel.\n3. Multiply both Fourier Transforms, point-wise.\n4. Compute the Inverse Fourier Transform of the result.\n\n_Brute-force 2D Gaussian vs. the convolution theorem_\n\n<img src=\"uploads/2022/71f4742163.png\" width=\"600\" height=\"609\" alt=\"\" />\n\n_Magic_\n\n<img src=\"uploads/2022/9fd4b3d82e.gif\" width=\"350\" height=\"196\" alt=\"\" />\n\n_Pros:_\n\n- Even more efficient: `O(n^2·log(n))`.\n- Works with any convolution kernel, separable or not.\n- Should the kernel be applied multiple times, the FFT of the kernel can be computed just once, and then be re-used.\n- The FFT/IFFT and the convolution product are cache-friendly.\n- The FFT/IFFT and the convolution product allow for multi-threading.\n\n_Cons:_\n\n- Definitely not easy to implement, unless you already own an FFT module that suits your needs.\n- The FFT operates on buffers with power-of-2 dimensions. This means that the input image (and the kernel) must be padded with zeroes to a larger size (i.e., extra setup time + memory).\n- Both the image and the kernel are transformed, which requires two auxiliary buffers instead of one.\n- Each FFT produces complex-number values, which doubles the memory usage of each auxiliary buffer.\n- The dynamic range of the FFT values is generally wilder than that of the original image. This requires a careful implementation, and the use of floating-point math regardless of the bit-depth and range of the input image.\n- This method doesn’t do any clamping at the boundaries of the image, producing a very annoying warp-around effect that may need special handling (e.g., more padding).\n\n_When should I use this method?_\n\n- Every time that a large generic convolution kernel (i.e., not a simple blur) is involved.\n- The most obvious examples I can think of in Maverick's are Glare & Bloom.\n\n## Can we do EVEN better?\n\nOh yes. We can do much better, at least in the very particular case of blur. I will talk about this in detail in a dedicated blog entry, at some point.\n\n**Some implementation remarks:**\n\nAll the algorithms presented above allow for **multi-threading**. Naturally, MT does not change the algorithm complexity, since the maximum number of threads is fixed, but you can get very decent speeds in practical cases if you combine sleek code with proper multi-threading. In the separable case (2), MT must be used twice. First for the rows, and then for the cols. In the FFT case (3), the FFT itself can be multi-threaded (in a rows/cols fashion as well). The huge point-wise product in frequency space can be MT’ed too.\n\nSince convolutions are usually applied on very large images, writing **cache-friendly code** can make a big difference. Assuming that the memory layout of your image/kernel is per rows, make sure to arrange your loops so memory accesses are as consecutive as possible. This is immediate for the loops that do a 1D convolution on each row. However, for the loops that do a 1D convolution on each column, it may help to use a local cache to transpose a column to a row back and forth.\n",
				"date_published": "2022-01-28T03:55:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/28/fast-convolutions-i.html",
				"tags": ["math","2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/17/pointspread-functions-convolutions.html",
				"title": "Point-Spread Functions \u0026 Convolutions (2014-07-14)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<p>One might explain what a <a href=\"https://en.wikipedia.org/wiki/Convolution\">convolution</a> is in many ways. However, in the field of image processing, there is an informal and very intuitive way to understand convolutions through the concept of <a href=\"https://en.wikipedia.org/wiki/Point_spread_function\">Point-Spread Functions</a> and their inverses.</p>\n\n<p>A PSF is an arbitrary description of the way in which a point spreads its energy around itself in 2D space.</p>\n\n<p><em>Classic PSFs: 1D box, 2D box, 1D Gaussian, 2D Gaussian</em></p>\n\n<p><img src=\"uploads/2022/abbfbd24d9.png\" width=\"166\" height=\"320\" alt=\"\" /></p>\n\n<p>Although this is not a mandatory requirement, the <em>integral</em> of a PSF usually equals 1, so no energy is gained or lost in the process. The above image does not match this requirement for the sake of visualization; the PSFs on the right column have been un-normalized for better visibility. On the other hand, the range of a PSF (how far away from the source point energy is allowed to reach) can be infinite. However, in most practical uses the range is finite, and usually as short as possible.</p>\n\n<p>So, a <code>PSF(x,y)</code> is a function <code>f:R^2-&gt;R</code> or, in the case of images, a finite/discrete real-value 2D matrix. For example, <code>PSF(x,y)=0.2</code> means that the point <code>P=(a,b)</code> sends 20% of its energy to point <code>Q=(a+x,b+y)</code>.</p>\n\n<p>If we apply the above PSFs to all the pixels in an image, this is what we get:</p>\n\n<p><em>Classic PSFs applied to an image</em></p>\n\n<p><img src=\"uploads/2022/12fa4bf752.png\" width=\"243\" height=\"320\" alt=\"\" /></p>\n\n<p><em>WARNING: Do not confuse this with a convolution. We’re not there yet.</em></p>\n\n<p>The inverse of a PSF (let’s use the term IPSF for now) is a description of what amount of energy a point receives from the points around itself in 2D space.</p>\n\n<p>So, an <code>IPSF(x,y)</code> is also a function <code>f:R^2-&gt;R</code> or, in the case of images, a finite/discrete real-value 2D matrix. For example, <code>IPSF(x,y)=0.2</code> means that the point <code>Q=(a,b)</code> receives 20% of the energy from point <code>P=(a+x,b+y)</code>.</p>\n\n<p>From here follows that a PSF and the corresponding IPSF are radially symmetric:</p>\n\n<p><code>IPSF(-x,-y) = PSF(x,y)</code></p>\n\n<p>If <code>P=(a,b)</code> spreads energy to <code>Q=(a+x,b+y)</code>, then <code>Q=(a’,b’)</code> gathers energy from <code>P=(a’-x,b’-y)</code>.</p>\n\n<p>Finally: <strong>a convolution is the result of applying the same IPSF to all the pixels of an image</strong>. Note that IPSF matrices are more commonly known as <em>convolution kernels</em>, or <em>filter kernels</em>.</p>\n\n<p>Conveniently enough, the PSFs displayed above are all radially symmetric with respect to themselves. As a matter of fact, it is true to most popular convolution kernels (e.g., 1D/2D box blur, 1D/2D Gaussian blur, …) that the PSF and the IPSF are identical. This makes the process of spreading/gathering energy equivalent in the cases presented above, but this is not true to other (more exotic) kernels.</p>\n\n<p>In the case of image convolutions, kernels are usually square matrices of dimensions DxD, where <code>D=(R+1+R)</code> and R is generally known as the radius of the kernel. This way, kernels have a central pixel. For instance, a kernel of R=3 (where each pixel is affected by neighbors never farther than 3px away) would be a 7x7 matrix.</p>\n\n<p>The convolution is a fundamental operation in Digital Image Processing, and most image filters (e.g., Gaussian Blur in Photoshop) are based on convolutions in one way or another.</p>\n\n<p><strong>Naive algorithm:</strong> A convolution is an operation that takes two discrete real-value matrices (<em>i.e.,</em> a luminance image and a convolution kernel) and makes the center of the kernel slide along each pixel in the image. At each pixel, the kernel is multiplied point-wise with all the pixels it covers, and the sum of these products is used to replace the original pixel value. Since this operation modifies pixels on the go, an auxiliary buffer is necessary.</p>\n\n<p>Let’s assume that the resolution of the image is WxH pixels, and the convolution kernel is a matrix of dimensions wxh. The convolution needs to run through WxH pixels and at each pixel, it must perform and add wxh products. This is as slow as <code>O(n^4)</code> = Terrible.</p>\n\n<p>As a matter of fact, the convolution of even a small kernel with a large image can take an eternity (literally) to compute using this naive solution. Fortunately, there is some mathematical trickery that we can take advantage of. More on fast convolutions in a future post.</p>\n\n<p><em>Bonus remark:</em> A particularly nice example of PSF is glare, which comes from the <em>Fraunhoffer diffraction</em> of the camera/eye aperture. Below you can see what happens when a glare PSF is applied to an HDR image. The actual implementation convolutes the IPSF (the radial-symmetric of the glare PSF) with the source HDR image.</p>\n\n<p><em>Glare applied to an Arion render</em></p>\n\n<p><img src=\"uploads/2022/d8b32e04d4.gif\" width=\"600\" height=\"355\" alt=\"\" /></p>\n\n<p><em>Typical glare PSF for a 6-blade iris</em></p>\n\n<p><img src=\"uploads/2022/42dd145d06.png\" width=\"256\" height=\"256\" alt=\"\" /></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\nOne might explain what a [convolution](https://en.wikipedia.org/wiki/Convolution) is in many ways. However, in the field of image processing, there is an informal and very intuitive way to understand convolutions through the concept of [Point-Spread Functions](https://en.wikipedia.org/wiki/Point_spread_function) and their inverses.\n\nA PSF is an arbitrary description of the way in which a point spreads its energy around itself in 2D space.\n\n_Classic PSFs: 1D box, 2D box, 1D Gaussian, 2D Gaussian_\n\n<img src=\"uploads/2022/abbfbd24d9.png\" width=\"166\" height=\"320\" alt=\"\" />\n\nAlthough this is not a mandatory requirement, the _integral_ of a PSF usually equals 1, so no energy is gained or lost in the process. The above image does not match this requirement for the sake of visualization; the PSFs on the right column have been un-normalized for better visibility. On the other hand, the range of a PSF (how far away from the source point energy is allowed to reach) can be infinite. However, in most practical uses the range is finite, and usually as short as possible.\n\nSo, a `PSF(x,y)` is a function `f:R^2->R` or, in the case of images, a finite/discrete real-value 2D matrix. For example, `PSF(x,y)=0.2` means that the point `P=(a,b)` sends 20% of its energy to point `Q=(a+x,b+y)`.\n\nIf we apply the above PSFs to all the pixels in an image, this is what we get:\n\n_Classic PSFs applied to an image_\n\n<img src=\"uploads/2022/12fa4bf752.png\" width=\"243\" height=\"320\" alt=\"\" />\n\n_WARNING: Do not confuse this with a convolution. We’re not there yet._\n\nThe inverse of a PSF (let’s use the term IPSF for now) is a description of what amount of energy a point receives from the points around itself in 2D space.\n\nSo, an `IPSF(x,y)` is also a function `f:R^2->R` or, in the case of images, a finite/discrete real-value 2D matrix. For example, `IPSF(x,y)=0.2` means that the point `Q=(a,b)` receives 20% of the energy from point `P=(a+x,b+y)`.\n\nFrom here follows that a PSF and the corresponding IPSF are radially symmetric:\n\n`IPSF(-x,-y) = PSF(x,y)`\n\nIf `P=(a,b)` spreads energy to `Q=(a+x,b+y)`, then `Q=(a’,b’)` gathers energy from `P=(a’-x,b’-y)`.\n\nFinally: **a convolution is the result of applying the same IPSF to all the pixels of an image**. Note that IPSF matrices are more commonly known as _convolution kernels_, or _filter kernels_.\n\nConveniently enough, the PSFs displayed above are all radially symmetric with respect to themselves. As a matter of fact, it is true to most popular convolution kernels (e.g., 1D/2D box blur, 1D/2D Gaussian blur, …) that the PSF and the IPSF are identical. This makes the process of spreading/gathering energy equivalent in the cases presented above, but this is not true to other (more exotic) kernels.\n\nIn the case of image convolutions, kernels are usually square matrices of dimensions DxD, where `D=(R+1+R)` and R is generally known as the radius of the kernel. This way, kernels have a central pixel. For instance, a kernel of R=3 (where each pixel is affected by neighbors never farther than 3px away) would be a 7x7 matrix.\n\nThe convolution is a fundamental operation in Digital Image Processing, and most image filters (e.g., Gaussian Blur in Photoshop) are based on convolutions in one way or another.\n\n**Naive algorithm:** A convolution is an operation that takes two discrete real-value matrices (_i.e.,_ a luminance image and a convolution kernel) and makes the center of the kernel slide along each pixel in the image. At each pixel, the kernel is multiplied point-wise with all the pixels it covers, and the sum of these products is used to replace the original pixel value. Since this operation modifies pixels on the go, an auxiliary buffer is necessary.\n\nLet’s assume that the resolution of the image is WxH pixels, and the convolution kernel is a matrix of dimensions wxh. The convolution needs to run through WxH pixels and at each pixel, it must perform and add wxh products. This is as slow as `O(n^4)` = Terrible.\n\nAs a matter of fact, the convolution of even a small kernel with a large image can take an eternity (literally) to compute using this naive solution. Fortunately, there is some mathematical trickery that we can take advantage of. More on fast convolutions in a future post.\n\n_Bonus remark:_ A particularly nice example of PSF is glare, which comes from the _Fraunhoffer diffraction_ of the camera/eye aperture. Below you can see what happens when a glare PSF is applied to an HDR image. The actual implementation convolutes the IPSF (the radial-symmetric of the glare PSF) with the source HDR image.\n\n_Glare applied to an Arion render_\n\n<img src=\"uploads/2022/d8b32e04d4.gif\" width=\"600\" height=\"355\" alt=\"\" />\n\n_Typical glare PSF for a 6-blade iris_\n\n<img src=\"uploads/2022/42dd145d06.png\" width=\"256\" height=\"256\" alt=\"\" />\n",
				"date_published": "2022-01-17T04:06:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/17/pointspread-functions-convolutions.html",
				"tags": ["render","math","2d","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/16/unbiased-spanningtree-generation.html",
				"title": "Unbiased spanning-tree generation",
				"content_html": "<p>Recently I&rsquo;ve done some micro-tests with spanning-trees in my spare time inspired by some talks I had with my good friend ditiem.</p>\n\n<p>The video below is my implementation of <a href=\"https://en.wikipedia.org/wiki/Maze_generation_algorithm\">Wilson&rsquo;s algorithm</a>.</p>\n\n<p>Spanning-trees can be visualized as classic mazes, which are always pretty to look at (bonus: rainbow floodfill). But let&rsquo;s forget about the maze itself. To me the majestuous beauty of this algorithm lies in the fact that it is <em>fully unbiased</em>:</p>\n\n<ul>\n<li>If the random numbers involved are uniformly distributed, it generates an also <em>uniformly distributed random sample</em> in the <em>space of all possible 2D-filling spanning-trees</em>.</li>\n<li><em>i.e.,</em> if the algorithm is called an infinite amount of times, <em>all</em> possible such mazes will be generated with <em>equal likelihood</em>.</li>\n</ul>\n\n<p><strong>Unbiasedness</strong> for the win.</p>\n\n<p>Some interesting remarks could be made about how to optimize the generation time stochastically. Let&rsquo;s save that discussion for another time&hellip;</p>\n\n<p><a href=\"https://www.youtube.com/watch?v=uaD1pkrS1wY\">Watch on Youtube</a></p>\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/ed291d75cb.mp4\" width=\"520\" height=\"520\" poster=\"https://www.brashandplucky.com/uploads/2022/5ca96a4051.png\" preload=\"none\"></video>\n",
				"content_text": "Recently I've done some micro-tests with spanning-trees in my spare time inspired by some talks I had with my good friend ditiem.\n\nThe video below is my implementation of [Wilson's algorithm](https://en.wikipedia.org/wiki/Maze_generation_algorithm).\n\nSpanning-trees can be visualized as classic mazes, which are always pretty to look at (bonus: rainbow floodfill). But let's forget about the maze itself. To me the majestuous beauty of this algorithm lies in the fact that it is _fully unbiased_:\n\n- If the random numbers involved are uniformly distributed, it generates an also _uniformly distributed random sample_ in the _space of all possible 2D-filling spanning-trees_.\n- _i.e.,_ if the algorithm is called an infinite amount of times, _all_ possible such mazes will be generated with _equal likelihood_.\n\n**Unbiasedness** for the win.\n\nSome interesting remarks could be made about how to optimize the generation time stochastically. Let's save that discussion for another time...\n\n[Watch on Youtube](https://www.youtube.com/watch?v=uaD1pkrS1wY)\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/ed291d75cb.mp4\" width=\"520\" height=\"520\" poster=\"https://www.brashandplucky.com/uploads/2022/5ca96a4051.png\" preload=\"none\"></video>\n",
				"date_published": "2022-01-16T17:16:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/16/unbiased-spanningtree-generation.html",
				"tags": ["math","2d"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/16/circular-and-radial.html",
				"title": "Circular \u0026 radial blur (2014-07-14)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<p>Circular and radial blur can be implemented in different ways. The method I am going to describe here is reasonably efficient, provided that there is a hyper-fast 1D box-based blur routine at one’s disposal (more on that in a future post). The algorithm is quite straightforward to implement, and also has the beautiful property of being able to do both circular and radial blur at the same time.</p>\n\n<p>I will work on grayscale images here, although as usual the process can by extended to color images by performing the same operations on the three R, G, and B channels.</p>\n\n<p>The key to circular/radial blur is to not work on the image space directly, but on a dual space, where cartesian co-ordinates are transformed to polar co-ordinates around the central pixel. Like this:</p>\n\n<p><em>Cartesian-to-polar transform</em></p>\n\n<p><img src=\"uploads/2022/2224df4a0d.png\" width=\"600\" height=\"110\" alt=\"\" /></p>\n\n<p>Each column in the transformed image is one of the ‘spokes’ that go from the center to one of the pixels at the perimeter in the original image. The length of the largest spoke is half a diagonal, while the perimeter of a WxH image has <code>2·(W+H-2)</code> pixels. So the transformed image is a buffer of dimensions <code>ceil(sqrt(W^2+H^2)/2)</code> and <code>2·(W+H-2)</code>.</p>\n\n<p>We also need an inverse transform that restores the original image from its polar form.</p>\n\n<p>Note that, for better results, the transform and also its inverse must do proper filtering. Since the spokes are diagonals that do not follow the arrangement of the pixels in the original image, the process of transforming and un-transforming is not exactly reciprocal. <em>i.e.,</em> un-transforming the transformed image does not restore the original image identically. In simpler words: this process adds some little blur due to filtering. However, this is ok, because we’re aiming at circular/radial blur after all.</p>\n\n<p>Below are the schematics of the type of filtering I am using in Maverick&rsquo;s API. When I sample a pixel centered at <code>(xm,ym)</code> along a spoke, I integrate the original image, constrained to a 1x1 square area. This integration simply takes the (up to) 4 overlapped pixels, and weighs each of them by the corresponding surface overlapping factor:</p>\n\n<p><em>Sample 1x1 px area</em></p>\n\n<p><img src=\"uploads/2022/85af42db00.png\" width=\"461\" height=\"322\" alt=\"\" /></p>\n\n<p>Note also that the inverse transform must ‘undo’ the filtering, by splatting contributions to the final image using the same surface overlapping factors.</p>\n\n<p>…and here comes the interesting part.</p>\n\n<ul>\n<li>If we 1D-blur the rows in the polar-space image, and then apply the inverse transform, we get a circular blur of the original image.</li>\n<li>If we 1D-blur the columns in the polar-space image, and then apply the inverse transform, we get a radial blur of the original image.</li>\n</ul>\n\n<p>A fast box-based 1D blur implementation can run in O(n), regardless of the radius of the blur. Let’s assume a square image of side S. The size of the transformed image is <code>2·(S+S-2)·sqrt(2)·S/2</code>, which means a quadratic complexity, or linear with respect to the number of pixels. The algorithm is made of fairly simple arithmetic operations, and allows for multi-threading.</p>\n\n<p>Here you can take a look at a couple of filtering examples taken from Maverick&rsquo;s Unit Testing System:</p>\n\n<p><em>Circular blur</em></p>\n\n<p><img src=\"uploads/2022/783f3b179f.gif\" width=\"600\" height=\"337\" alt=\"\" /></p>\n\n<p><em>Radial blur</em></p>\n\n<p><img src=\"uploads/2022/f74e2ef315.gif\" width=\"600\" height=\"337\" alt=\"\" /></p>\n\n<p>Some bonus remarks:</p>\n\n<ul>\n<li>If the amount of blur becomes progressively small as you approach the center, radial blur becomes lens blur.</li>\n<li>If the amount of blur used in radial blur is different for each color component, you get chromatic aberration.</li>\n<li>If you work with spectral colors, instead of RGB, chromatic aberration looks great even when the blur stretches colors along long streaks.</li>\n</ul>\n\n<p><em>Lens blur &amp; heavy chromatic aberration</em></p>\n\n<p><img src=\"uploads/2022/6c6105cdb8.gif\" width=\"600\" height=\"337\" alt=\"\" /></p>\n\n<p>Some more final remarks:</p>\n\n<ul>\n<li>In general, blur operations are clamped to the boundaries of the buffer they operate on. However, in the case of circular blur, one must warp-around from the first to the last spoke.</li>\n<li>It is not really necessary to create the transformed polar image, which is (much) larger than the original. One can feed the 1D blur with transformed pixels directly, and save some memory. Doing so doesn’t cause a performance penalty, because the algorithm runs through each spoke only once.</li>\n</ul>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\nCircular and radial blur can be implemented in different ways. The method I am going to describe here is reasonably efficient, provided that there is a hyper-fast 1D box-based blur routine at one’s disposal (more on that in a future post). The algorithm is quite straightforward to implement, and also has the beautiful property of being able to do both circular and radial blur at the same time.\n\nI will work on grayscale images here, although as usual the process can by extended to color images by performing the same operations on the three R, G, and B channels.\n\nThe key to circular/radial blur is to not work on the image space directly, but on a dual space, where cartesian co-ordinates are transformed to polar co-ordinates around the central pixel. Like this:\n\n_Cartesian-to-polar transform_\n\n<img src=\"uploads/2022/2224df4a0d.png\" width=\"600\" height=\"110\" alt=\"\" />\n\nEach column in the transformed image is one of the ‘spokes’ that go from the center to one of the pixels at the perimeter in the original image. The length of the largest spoke is half a diagonal, while the perimeter of a WxH image has `2·(W+H-2)` pixels. So the transformed image is a buffer of dimensions `ceil(sqrt(W^2+H^2)/2)` and `2·(W+H-2)`.\n\nWe also need an inverse transform that restores the original image from its polar form.\n\nNote that, for better results, the transform and also its inverse must do proper filtering. Since the spokes are diagonals that do not follow the arrangement of the pixels in the original image, the process of transforming and un-transforming is not exactly reciprocal. _i.e.,_ un-transforming the transformed image does not restore the original image identically. In simpler words: this process adds some little blur due to filtering. However, this is ok, because we’re aiming at circular/radial blur after all.\n\nBelow are the schematics of the type of filtering I am using in Maverick's API. When I sample a pixel centered at `(xm,ym)` along a spoke, I integrate the original image, constrained to a 1x1 square area. This integration simply takes the (up to) 4 overlapped pixels, and weighs each of them by the corresponding surface overlapping factor:\n\n_Sample 1x1 px area_\n\n<img src=\"uploads/2022/85af42db00.png\" width=\"461\" height=\"322\" alt=\"\" />\n\nNote also that the inverse transform must ‘undo’ the filtering, by splatting contributions to the final image using the same surface overlapping factors.\n\n…and here comes the interesting part.\n\n- If we 1D-blur the rows in the polar-space image, and then apply the inverse transform, we get a circular blur of the original image.\n- If we 1D-blur the columns in the polar-space image, and then apply the inverse transform, we get a radial blur of the original image.\n\nA fast box-based 1D blur implementation can run in O(n), regardless of the radius of the blur. Let’s assume a square image of side S. The size of the transformed image is `2·(S+S-2)·sqrt(2)·S/2`, which means a quadratic complexity, or linear with respect to the number of pixels. The algorithm is made of fairly simple arithmetic operations, and allows for multi-threading.\n\nHere you can take a look at a couple of filtering examples taken from Maverick's Unit Testing System:\n\n_Circular blur_\n\n<img src=\"uploads/2022/783f3b179f.gif\" width=\"600\" height=\"337\" alt=\"\" />\n\n_Radial blur_\n\n<img src=\"uploads/2022/f74e2ef315.gif\" width=\"600\" height=\"337\" alt=\"\" />\n\nSome bonus remarks:\n\n- If the amount of blur becomes progressively small as you approach the center, radial blur becomes lens blur.\n- If the amount of blur used in radial blur is different for each color component, you get chromatic aberration.\n- If you work with spectral colors, instead of RGB, chromatic aberration looks great even when the blur stretches colors along long streaks.\n\n_Lens blur & heavy chromatic aberration_\n\n<img src=\"uploads/2022/6c6105cdb8.gif\" width=\"600\" height=\"337\" alt=\"\" />\n\nSome more final remarks:\n\n- In general, blur operations are clamped to the boundaries of the buffer they operate on. However, in the case of circular blur, one must warp-around from the first to the last spoke.\n- It is not really necessary to create the transformed polar image, which is (much) larger than the original. One can feed the 1D blur with transformed pixels directly, and save some memory. Doing so doesn’t cause a performance penalty, because the algorithm runs through each spoke only once.\n",
				"date_published": "2022-01-16T16:56:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/16/circular-and-radial.html",
				"tags": ["math","2d","coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/15/randomwalk-sss.html",
				"title": "Random-walk SSS",
				"content_html": "<p>Arion, predecesor of Maverick Render, was already doing &ldquo;random-walk&rdquo; volume and SSS rendering long before the term random-walk even became trendy.</p>\n\n<p>This is a graph plotted by some simulation code I prototyped way back during the R&amp;D phase (circa 2014). Each wavelength (simplified to RGB in the plot) traverses the media with different stochastic statistics.</p>\n\n<p><img src=\"uploads/2022/a0dd1496bf.jpg\" width=\"600\" height=\"450\" alt=\"\" /></p>\n",
				"content_text": "Arion, predecesor of Maverick Render, was already doing \"random-walk\" volume and SSS rendering long before the term random-walk even became trendy.\n\nThis is a graph plotted by some simulation code I prototyped way back during the R&D phase (circa 2014). Each wavelength (simplified to RGB in the plot) traverses the media with different stochastic statistics.\n\n<img src=\"uploads/2022/a0dd1496bf.jpg\" width=\"600\" height=\"450\" alt=\"\" />\n",
				"date_published": "2022-01-15T01:10:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/15/randomwalk-sss.html",
				"tags": ["render","math","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/legendre-polynomials-because.html",
				"title": "Legendre polynomials",
				"content_html": "<p><a href=\"https://en.wikipedia.org/wiki/Legendre_polynomials\">Legendre polynomials</a>. Because&hellip; Why not?</p>\n\n<p><img src=\"uploads/2022/412a9abc3b.png\" width=\"256\" height=\"256\" alt=\"\" /></p>\n",
				"content_text": "[Legendre polynomials](https://en.wikipedia.org/wiki/Legendre_polynomials). Because... Why not?\n\n<img src=\"uploads/2022/412a9abc3b.png\" width=\"256\" height=\"256\" alt=\"\" />\n",
				"date_published": "2022-01-14T23:39:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/legendre-polynomials-because.html",
				"tags": ["math"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/215738.html",
				"title": "Anatomy of a PE (2012-10-30)",
				"content_html": "\n\n<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<h2 id=\"preamble\">Preamble</h2>\n\n<p>Executables (EXE/DLL) in Microsoft Windows make use of a format called PE (<em>Portable Executable</em>).</p>\n\n<p>A PE image typically lies in the hard drive as a <em>.exe</em> or <em>.dll</em> file, and is loaded by the <em>Windows Loader</em> into the RAM of the system when a process is created from that file (<em>e.g.</em>, by double-clicking the <em>.exe</em>). So, basically, we have two different states:</p>\n\n<ul>\n<li><em>Physical</em> PE image file.</li>\n<li><em>Loaded</em> PE image (running process).</li>\n</ul>\n\n<h2 id=\"layout\">Layout</h2>\n\n<p>The layout of a PE image contains the following elements:</p>\n\n<ul>\n<li>A header, glued at the first byte of the file.</li>\n<li>A list of consecutive sections.</li>\n<li>An optional chunk of trailing (unused) bytes.</li>\n</ul>\n\n<p>These elements are concatenated, and there may or may not be padding space between them. This padding space, if present, is usually filled with <code>0x00</code> bytes.</p>\n\n<p>It is possible to add trailing bytes to a PE image. Actually, those files will simply lie there doing nothing. However, some types of processes such as package installers can use the trailing area to add some payload that the process will use (<em>e.g.,</em> decrypt, uncompress, …) at some point.</p>\n\n<h2 id=\"the-pe-header\">The PE Header</h2>\n\n<p>The PE header contains a wealth of information, structured in a fixed way.</p>\n\n<p>Among the many details about the PE that you can gather from the header, there’s the <em>list of sections</em>. The list of sections contains the name, position, and size of each section in the PE, both in physical file format, and when loaded in memory as a process.</p>\n\n<p>Many other things are described by the header, such as the address of the <em>Entry Point</em> (the instruction where code execution must start), and the address of some <em>data directories</em>. It is in these data directories where you can find the list of DLLs the PE depends on, the list of functions exported by the PE, etc…</p>\n\n<p>The PE Header is usually <code>0x0400</code> (1024) bytes long. The data structures contained in it are defined in <code>winnt.h</code>, and they are always the same (so you can assume offsets and such). One must pay attention, though, to the fact that depending on whether the process is 32-bit or 64-bit, some pieces of the header will be PE32 or PE64. One can tell whether the process is 32-bit or 64-bit by checking the header flags (which are found at a location which is common to PE32 and PE64 headers).</p>\n\n<h2 id=\"rvas-and-vas\">RVAs and VAs</h2>\n\n<p>In general, the addresses found in the PE header are given as RVAs (<em>Relative Virtual Addresses</em>). An RVA is an offset relative to the first byte of the PE image. Assuming that you know the location in memory of the first byte of the PE image (the image base pointer), then the relationship between a VA (<em>Virtual Address</em>) and its corresponding RVA is given by:</p>\n\n<p><code>VA = ( base + RVA )</code></p>\n\n<p><code>RVA = ( VA - base )</code></p>\n\n<p>If the PE is a physical file, then the base pointer is simply the start of the file. However, if the PE has been loaded as a process, then the base address can be found in several ways that should (in theory) match:</p>\n\n<ul>\n<li>The value returned by <code>GetModuleHandle(0)</code> is, in fact, the base pointer.</li>\n<li>The <code>HINSTANCE</code> received by <code>WinMain</code> is, in fact, the base pointer.</li>\n<li>The Windows Loader stores the base pointer at the PE header on load, in the <code>BaseOfImage</code> field.</li>\n</ul>\n\n<p>The PE header is glued at this location in memory. So, in run-time, one can use this knowledge to do PE-related operations such as leap-frogging through the PE sections. A typical anti-cracking use is to run a CRC32 on the code section in order to display a <em>badboy</em> message if the executable code has been patched, infected, or tampered with.</p>\n\n<h2 id=\"physical-vs-loaded-state\">Physical vs. loaded state</h2>\n\n<p>The anatomy of the PE image is different, yet quite similar, in both (physical vs. loaded) PE states.</p>\n\n<p>When a PE is loaded as a process, the PE image file gets chopped in sections, and these sections get relocated (copied) in memory:</p>\n\n<ul>\n<li>In both states, the PE image forms a block of contiguous bytes.</li>\n<li>In both states, the header and the sections are found in the same order.</li>\n<li>The amount of padding between sections usually differs.</li>\n</ul>\n\n<p>In their physical file form, PE sections do not need to have any particular padding, so EXE/DLL files can be as small as possible. On the other hand, when loaded, each PE section occupies a certain number of whole memory pages with certain permissions (execution, read-only, etc…). The amount of padding for the start address and the size of each section is given by the PE header.</p>\n\n<p>It is important to note that this relocation procedure simply adds some padding between sections. The section chunks themselves remain identical, and are a straight copy of the original bytes found in the physical file. The only exceptions are some areas which get initialised by the Windows Loader on load (<em>e.g.,</em> some fields in the PE header, the IAT, …). I will talk about these in a future post.</p>\n\n<p><strong>[EDIT] I have made significant progress in this area in the 10+ years since I wrote this. But Part II of this post never saw the light of day. Spare time is scarce.</strong></p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\n## Preamble\n\nExecutables (EXE/DLL) in Microsoft Windows make use of a format called PE (_Portable Executable_).\n\nA PE image typically lies in the hard drive as a _.exe_ or _.dll_ file, and is loaded by the _Windows Loader_ into the RAM of the system when a process is created from that file (_e.g._, by double-clicking the _.exe_). So, basically, we have two different states:\n\n- _Physical_ PE image file.\n- _Loaded_ PE image (running process).\n\n## Layout\n\nThe layout of a PE image contains the following elements:\n\n- A header, glued at the first byte of the file.\n- A list of consecutive sections.\n- An optional chunk of trailing (unused) bytes.\n\nThese elements are concatenated, and there may or may not be padding space between them. This padding space, if present, is usually filled with `0x00` bytes.\n\nIt is possible to add trailing bytes to a PE image. Actually, those files will simply lie there doing nothing. However, some types of processes such as package installers can use the trailing area to add some payload that the process will use (_e.g.,_ decrypt, uncompress, …) at some point.\n\n## The PE Header\n\nThe PE header contains a wealth of information, structured in a fixed way.\n\nAmong the many details about the PE that you can gather from the header, there’s the _list of sections_. The list of sections contains the name, position, and size of each section in the PE, both in physical file format, and when loaded in memory as a process.\n\nMany other things are described by the header, such as the address of the _Entry Point_ (the instruction where code execution must start), and the address of some _data directories_. It is in these data directories where you can find the list of DLLs the PE depends on, the list of functions exported by the PE, etc…\n\nThe PE Header is usually `0x0400` (1024) bytes long. The data structures contained in it are defined in `winnt.h`, and they are always the same (so you can assume offsets and such). One must pay attention, though, to the fact that depending on whether the process is 32-bit or 64-bit, some pieces of the header will be PE32 or PE64. One can tell whether the process is 32-bit or 64-bit by checking the header flags (which are found at a location which is common to PE32 and PE64 headers).\n\n## RVAs and VAs\n\nIn general, the addresses found in the PE header are given as RVAs (_Relative Virtual Addresses_). An RVA is an offset relative to the first byte of the PE image. Assuming that you know the location in memory of the first byte of the PE image (the image base pointer), then the relationship between a VA (_Virtual Address_) and its corresponding RVA is given by:\n\n`VA = ( base + RVA )`\n\n`RVA = ( VA - base )`\n\nIf the PE is a physical file, then the base pointer is simply the start of the file. However, if the PE has been loaded as a process, then the base address can be found in several ways that should (in theory) match:\n\n- The value returned by `GetModuleHandle(0)` is, in fact, the base pointer.\n- The `HINSTANCE` received by `WinMain` is, in fact, the base pointer.\n- The Windows Loader stores the base pointer at the PE header on load, in the `BaseOfImage` field.\n\nThe PE header is glued at this location in memory. So, in run-time, one can use this knowledge to do PE-related operations such as leap-frogging through the PE sections. A typical anti-cracking use is to run a CRC32 on the code section in order to display a _badboy_ message if the executable code has been patched, infected, or tampered with.\n\n## Physical vs. loaded state\n\nThe anatomy of the PE image is different, yet quite similar, in both (physical vs. loaded) PE states.\n\nWhen a PE is loaded as a process, the PE image file gets chopped in sections, and these sections get relocated (copied) in memory:\n\n- In both states, the PE image forms a block of contiguous bytes.\n- In both states, the header and the sections are found in the same order.\n- The amount of padding between sections usually differs.\n\nIn their physical file form, PE sections do not need to have any particular padding, so EXE/DLL files can be as small as possible. On the other hand, when loaded, each PE section occupies a certain number of whole memory pages with certain permissions (execution, read-only, etc…). The amount of padding for the start address and the size of each section is given by the PE header.\n\nIt is important to note that this relocation procedure simply adds some padding between sections. The section chunks themselves remain identical, and are a straight copy of the original bytes found in the physical file. The only exceptions are some areas which get initialised by the Windows Loader on load (_e.g.,_ some fields in the PE header, the IAT, …). I will talk about these in a future post.\n\n**[EDIT] I have made significant progress in this area in the 10+ years since I wrote this. But Part II of this post never saw the light of day. Spare time is scarce.**\n",
				"date_published": "2022-01-14T22:57:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/215738.html",
				"tags": ["coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/215617.html",
				"title": "Code fortification (2012-10-17)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<p>Lately, I’ve been reading a lot about code fortification, and low-level debugging. Here are some of the most interesting links I’ve found. They’re all worth reading if you fancy hardcore OS internals and such:</p>\n\n<p><a href=\"https://docs.microsoft.com/en-us/archive/msdn-magazine/2002/february/inside-windows-win32-portable-executable-file-format-in-detail\">An In-Depth Look into the Win32 Portable Executable File Format</a></p>\n\n<p><a href=\"https://docs.microsoft.com/en-us/previous-versions/ms809762(v=msdn.10)?redirectedfrom=MSDN\">Peering Inside the PE: A Tour of the Win32 Portable Executable File Format</a></p>\n\n<p><a href=\"https://code.google.com/archive/p/corkami/wikis/PE.wiki\">The Portable Executable Format on Windows</a></p>\n\n<p><a href=\"https://www.codeproject.com/Articles/43682/Writing-a-basic-Windows-debugger\">Writing a basic Windows debugger (Part I)</a></p>\n\n<p><a href=\"https://www.codeproject.com/Articles/132742/Writing-Windows-Debugger-Part-2\">Writing a basic Windows debugger (Part II)</a></p>\n\n<p><a href=\"https://www.codeproject.com/Articles/30815/An-Anti-Reverse-Engineering-Guide\">An Anti-Reverse Engineering Guide</a></p>\n\n<p><a href=\"https://www.codeproject.com/Articles/29469/Introduction-Into-Windows-Anti-Debugging\">Introduction Into Windows Anti-Debugging</a></p>\n\n<p><a href=\"http://sandsprite.com/CodeStuff/Understanding_imports.html\">Understanding the Import Address Table</a></p>\n\n<p><a href=\"https://pferrie.tripod.com/papers/unpackers.pdf\">Anti-unpacker tricks</a></p>\n\n<p>This all brings me back to my teenager years coding in x86 assembly language.</p>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\nLately, I’ve been reading a lot about code fortification, and low-level debugging. Here are some of the most interesting links I’ve found. They’re all worth reading if you fancy hardcore OS internals and such:\n\n[An In-Depth Look into the Win32 Portable Executable File Format](https://docs.microsoft.com/en-us/archive/msdn-magazine/2002/february/inside-windows-win32-portable-executable-file-format-in-detail)\n\n[Peering Inside the PE: A Tour of the Win32 Portable Executable File Format](https://docs.microsoft.com/en-us/previous-versions/ms809762(v=msdn.10)?redirectedfrom=MSDN)\n\n[The Portable Executable Format on Windows](https://code.google.com/archive/p/corkami/wikis/PE.wiki)\n\n[Writing a basic Windows debugger (Part I)](https://www.codeproject.com/Articles/43682/Writing-a-basic-Windows-debugger)\n\n[Writing a basic Windows debugger (Part II)](https://www.codeproject.com/Articles/132742/Writing-Windows-Debugger-Part-2)\n\n[An Anti-Reverse Engineering Guide](https://www.codeproject.com/Articles/30815/An-Anti-Reverse-Engineering-Guide)\n\n[Introduction Into Windows Anti-Debugging](https://www.codeproject.com/Articles/29469/Introduction-Into-Windows-Anti-Debugging)\n\n[Understanding the Import Address Table](http://sandsprite.com/CodeStuff/Understanding_imports.html)\n\n[Anti-unpacker tricks](https://pferrie.tripod.com/papers/unpackers.pdf)\n\nThis all brings me back to my teenager years coding in x86 assembly language.\n",
				"date_published": "2022-01-14T22:56:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/215617.html",
				"tags": ["coding"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/212534.html",
				"title": "Index-Of-Refraction (2011-10-15)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<p>This all below is stuff that kids study in school. :)</p>\n\n<p>Below is a visualization of the behavior of a ray of light as it hits a dielectric interface.</p>\n\n<p>Some key phenomena which show up in the video are:</p>\n\n<ul>\n<li>The <em>Fresnel</em> term (reflection vs. refraction).</li>\n<li>The <em>Index of Refraction</em>.</li>\n<li>The <em>critical angle</em>.</li>\n<li><em>Total Internal Reflection</em> (TIR).</li>\n<li>As nd increases the Index of Refraction becomes higher, and so does the Fresnel term, which defines the proportion between reflected and refracted light. The critical angle becomes higher too, so there is more Total Internal Reflection.</li>\n</ul>\n\n<p>When a ray of light hits an interface (assuming an ideal surface), all incident light must be either reflected or refracted. The Fresnel term (controlled by nd) tells how much light is reflected at a given incident angle. All the light that is not reflected is refracted, so both amounts (reflection and refraction) always add up to the total amount of incident light.</p>\n\n<p>The Fresnel term approaches 1 at grazing angles (all light is reflected and nothing is refracted, regardless of nd) and is low (the lower the smaller the nd) at perpendicular angles (more light is refracted).</p>\n\n<p>As a rule of thumb:</p>\n\n<ul>\n<li>The lower the nd, the lower the IOR, and the more transparent the surface (more glass/liquid-like).</li>\n<li>The higher the nd, the higher the IOR, and the more reflective the surface (more metallic/mirror-like).</li>\n</ul>\n\n<p>For example:</p>\n\n<ul>\n<li>Void: nd=1.</li>\n<li>Air: nd=1.1.</li>\n<li>Water: nd=1.33.</li>\n<li>Glass: nd=1.51.</li>\n<li>Diamond: nd=2.5.</li>\n<li>Metals: nd=20+. (approx. complex IOR).</li>\n<li>Ideal mirror: nd=infinity.</li>\n</ul>\n\n<p>When a ray of light enters a medium with an nd lower than the nd of the previous medium, there is an angle at which the Fresnel term becomes 1 and beyond which light won’t refract anymore. This angle is called <em>critical angle</em>, and beyond it, the surface behaves like a perfect mirror, reflecting back all incident light. This effect is called <em>Total Internal Reflection</em> (TIR).</p>\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/0f9a215aa8.mov\" width=\"640\" height=\"213\" poster=\"https://www.brashandplucky.com/uploads/2022/83b8d73cd8.png\" preload=\"none\"></video>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\nThis all below is stuff that kids study in school. :)\n\nBelow is a visualization of the behavior of a ray of light as it hits a dielectric interface.\n\nSome key phenomena which show up in the video are:\n\n- The _Fresnel_ term (reflection vs. refraction).\n- The _Index of Refraction_.\n- The _critical angle_.\n- _Total Internal Reflection_ (TIR).\n- As nd increases the Index of Refraction becomes higher, and so does the Fresnel term, which defines the proportion between reflected and refracted light. The critical angle becomes higher too, so there is more Total Internal Reflection.\n\nWhen a ray of light hits an interface (assuming an ideal surface), all incident light must be either reflected or refracted. The Fresnel term (controlled by nd) tells how much light is reflected at a given incident angle. All the light that is not reflected is refracted, so both amounts (reflection and refraction) always add up to the total amount of incident light.\n\nThe Fresnel term approaches 1 at grazing angles (all light is reflected and nothing is refracted, regardless of nd) and is low (the lower the smaller the nd) at perpendicular angles (more light is refracted).\n\nAs a rule of thumb:\n\n- The lower the nd, the lower the IOR, and the more transparent the surface (more glass/liquid-like).\n- The higher the nd, the higher the IOR, and the more reflective the surface (more metallic/mirror-like).\n\nFor example:\n\n- Void: nd=1.\n- Air: nd=1.1.\n- Water: nd=1.33.\n- Glass: nd=1.51.\n- Diamond: nd=2.5.\n- Metals: nd=20+. (approx. complex IOR).\n- Ideal mirror: nd=infinity.\n\nWhen a ray of light enters a medium with an nd lower than the nd of the previous medium, there is an angle at which the Fresnel term becomes 1 and beyond which light won’t refract anymore. This angle is called _critical angle_, and beyond it, the surface behaves like a perfect mirror, reflecting back all incident light. This effect is called _Total Internal Reflection_ (TIR).\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/0f9a215aa8.mov\" width=\"640\" height=\"213\" poster=\"https://www.brashandplucky.com/uploads/2022/83b8d73cd8.png\" preload=\"none\"></video>\n",
				"date_published": "2022-01-14T22:25:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/212534.html",
				"tags": ["render","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/edit-this-post.html",
				"title": "The cosine lobe (2021-10-12)",
				"content_html": "<p><strong>[EDIT] This post was migrated from my blog from 2011&hellip;</strong></p>\n\n<p>Testing image/video uploads.</p>\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/cf29dbceb3.mov\" width=\"512\" height=\"256\" poster=\"https://www.brashandplucky.com/uploads/2022/968506d58e.png\" preload=\"none\"></video>\n",
				"content_text": "**[EDIT] This post was migrated from my blog from 2011...**\n\nTesting image/video uploads.\n\n<video controls=\"controls\" playsinline=\"playsinline\" src=\"https://www.brashandplucky.com/uploads/2022/cf29dbceb3.mov\" width=\"512\" height=\"256\" poster=\"https://www.brashandplucky.com/uploads/2022/968506d58e.png\" preload=\"none\"></video>\n",
				"date_published": "2022-01-14T22:12:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/edit-this-post.html",
				"tags": ["render","physics"]
			},
			{
				"id": "http://brashandplucky.micro.blog/2022/01/14/hello-again.html",
				"title": "Hello again!",
				"content_html": "<p>Like most &ldquo;computer people&rdquo; I&rsquo;ve had some attempts over the years to manage a personal website, then a blog, then various social media accounts, etc&hellip; to quickly realize with each attempt that I don&rsquo;t have the time, and to some extent, the will or need to keep on posting. I am always way too busy for <em>non-productive</em> stuff.</p>\n\n<p>On the other hand, pretty much like GitHub itself, there is some true value in archiving and organizing images, results, etc&hellip; of projects you&rsquo;ve worked on, even if just to build some sort of &ldquo;historical archive&rdquo; of your journey as a researcher.</p>\n\n<p>I am certain that I won&rsquo;t be updating this micro-blog often (if at all). Mostly because my R&amp;D and management work at <a href=\"https://randomcontrol.com\">RandomControl</a> (<a href=\"https://maverickrender.com\">Maverick Render</a>) leaves me with basically no spare time. But I will try to &ldquo;now and then&rdquo; drop here some 3D/2D/physics code experiments and such.</p>\n\n<p>I will start by transporting (some of) my very old <a href=\"https://chemaguerra.com\">chemaguerra.com</a> blog from 2011 to micro-posts here.</p>\n\n<p>Godspeed!</p>\n",
				"content_text": "Like most \"computer people\" I've had some attempts over the years to manage a personal website, then a blog, then various social media accounts, etc... to quickly realize with each attempt that I don't have the time, and to some extent, the will or need to keep on posting. I am always way too busy for _non-productive_ stuff.\n\nOn the other hand, pretty much like GitHub itself, there is some true value in archiving and organizing images, results, etc... of projects you've worked on, even if just to build some sort of \"historical archive\" of your journey as a researcher.\n\nI am certain that I won't be updating this micro-blog often (if at all). Mostly because my R&D and management work at [RandomControl](https://randomcontrol.com) ([Maverick Render](https://maverickrender.com)) leaves me with basically no spare time. But I will try to \"now and then\" drop here some 3D/2D/physics code experiments and such.\n\nI will start by transporting (some of) my very old [chemaguerra.com](https://chemaguerra.com) blog from 2011 to micro-posts here.\n\nGodspeed!\n",
				"date_published": "2022-01-14T19:35:00+02:00",
				"url": "https://www.brashandplucky.com/2022/01/14/hello-again.html",
				"tags": ["personal"]
			}
	]
}
